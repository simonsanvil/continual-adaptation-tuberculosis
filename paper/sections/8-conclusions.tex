\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../imagenes/}}}

\begin{document}

\chapter{Conclusions} \label{chap:conclusions} \info{9-10 páginas}
    

    Discussion about the results obtained and implications in the context of the project, limitations of the proposed system, future work, etc.
    

    \section{Main Implications} \label{conclusions:implications} \info{0.65 pág}

    \section{Limitations of the System} \label{conclusions:limitations} \info{0.5 pág}
    
    \section{Future Work} \label{conclusions:future_work} \info{~2.75 pág}
    % \addcontentsline{toc}{section}{Future Work}

    The following section\dots

    \subsection{
        Implementing new adaptation strategies to the system 
    } \label{conclusions:future_work:adaptation_strategies}\info{0.5 pág}

    Furthermore, we could integrate more hardware-related optimization techniques like quantization and network pruning methods that aim to reduce the size of a DNN and/or reduce its computational cost. The way they go about doing that is different. Network pruning removes redundant parameters of the neural network, while quantization reduces the precision of the weights and activations of the model (e.g., from 32-bit floating-point numbers to 8-bit integers).
    
    Rather than using these techniques to improve the model's performance on a particular task, these would be used to possibly reduce the computational or memory cost of the system. The way we would propose such an adaptation tactic would be to continually evaluate different versions (quantized, not quantized, pruned, not pruned) of the model to determine which one is the most suitable for the given task, and under what circumstances.

    For example, we could have a model that is trained on a particular task and then quantized to reduce its computational cost. The system would then evaluate the performance of the quantized model on the task under all monitored aspects and determine whether the performance drop is relevant enough to warrant the use of the quantized model and reduce the computational cost of the overall system.

    Cite \cite{carreira-perpinan_model_2017} and \cite{han_deep_2016}, \cite{carreira-perpinan_compression_2018}

    \subsection{
        Improving the explainability of the continual learning process 
    } \label{conclusions:future_work:explainability}\info{0.5 pág}

    Anytime a model is trained on a particular dataset, any biases present on that dataset are also introduced to the model. In any high-stakes environment, the researchers who design these models should be aware of the biases present in the data and how they might affect the model's predictions and consider them in the decision-making process.

    Furthermore, this aspect is especially crucial in the healthcare sector, where legislation and ethical guidelines stress the importance of AI systems being transparent \cite{noauthor_ethics_2019,eu_aiact_2023}. Healthcare professionals have the moral and legal obligation to be able to explain the decisions made by the AI systems they use to the relevant stakeholders, and the latter also has the right to know how the decisions that affect them are made.
    
    In that regard, one of the main limitations of the system proposed here is the lack of explainability of the continual learning process, where a model that is designed initially to be as transparent as possible might gradually lose explainability power as it adapts to new data distributions over time.
    
    
    
    Methods such as TRAK \cite{park_trak_2023} have been proposed as a way to improve the explainability of Deep Learning models by providing a way to trace the predictions of a model to individual instances of the training data in a concept known as \textit{data attribution}, which has been proven useful in improving sample selection for active learning approaches \cite{park_trak_2023, holzmuller_framework_2023, liu_influence_2021}.

    \subsection{Adapting the system to run on a federated platform} \label{conclusions:future_work:federated} \info{1.75 pág}

    Efforts such as \cite{joshi_federated_2022} have demonstrated the benefits and potential of taking a federated learning approach to training machine learning models in the healthcare industry. A future proposal to improve the system presented in this work would be to adapt the current platform to one where the models are trained in a federated fashion.
    
    We would propose a system where each research laboratory trains an instance of a (previously agreed) global model with its own local (private) data and shares the trained weights with a central server. The new platform would be mainly used to perform inference on public data, while each client could have an instance of the annotation frontend and backend, including the active learning framework, running locally to annotate their data. 
    
    The motivation for this comes from the fact that is very difficult to convince partners (even within the same project organization) to share their data. In the healthcare industry, data is considered to be a very valuable asset, with a high cost to obtain and annotate, which is why it is often used as a bargaining chip in negotiations between companies and research organizations \todo{include citation}.

    \todo[inline]{Show a diagram of the proposed federated learning system.}

    \clearpage

    \section{Further Research Directions} \label{conclusions:research_directions} \info{~4.5 pág}

    The following are some of the recent and upcoming research directions in the field of machine learning that we consider to be relevant to the work presented in this thesis.
    
    Unlike the previous section, which focused on the limitations of the proposed system and detailed ideas about how to address or improve them in the immediate future, this section takes a broader view, focusing on gaps in the current state-of-the-art and what might be considered to be the next steps in the field.
    
    The idea is that these directions could used as a starting point for future research (i.e. a Ph.D. thesis) on the topics presented in this thesis, either as a continuation of some of the work here or as a completely new approach to the problem. We make no claims about the feasibility of these ideas but rather present why I consider them to be interesting for further research and/or discussion.


    \subsection{Future directions in Tuberculosis AI Research} \label{conclusions:future_work:tb_ai} \info{<1 pág}
    
    Detection will soon be a solved problem thanks to NAATs (or rather, more of a money problem than a technical one). Thus, the emphasis should be less on CV-based detectors and more to advances in drug discovery, finding new biomarkers, etc. \dots

    \clearpage


    \subsection{Scalable Adaptability through Mixtures of Experts} \label{conclusions:research_directions:moes} \info{1.5 pág}

    Mixture of experts (MoE) systems are a type of ensemble model that combines the predictions of multiple models to obtain a final prediction. The difference between MoEs and other ensemble models is that the predictions of the individual `experts' are combined using a gating function that adapts to the given data point and dynamically determines the weight of each model in the final prediction \cite{chen_towards_2022}.

    MoEs are really powerful systems. They have been shown to be able to learn complex multimodal distributions and have been used in a wide variety of applications, including object detection, language modeling, machine translation, and even multiomics \cite{hwang_tutel_2023,mustafa_multimodal_2022,shazeer_outrageously_2017, minoura_scmm_2021}.
    
    But by far, the biggest advantage of MoEs is that each expert model can be deployed independently, allowing for a more flexible, modular system capable of being distributed and data-parallelized among different hardware resources. 

    The computational sparsity of these systems has shown the capacity of MoEs to scale DNN models to outrageous amounts of parameters. Recently, researchers at Google Brain presented a MoE architecture called Switch Transformers \cite{fedusSwitchTransformersScaling2022a} that allows language models - AI systems that can generate text - to scale to even a trillion parameters at a constant computational cost.
    
    Furthermore, systems that take advantage of MoEs to scale LLMs (Large Language Models) already exist and have been deployed to production for applications as big as ChatGPT (GPT-4 is thought to be a MoE with over 8-billion parameters \todo{add citation}).
    
    In the context of the area of this work, we consider that MoEs could be used to create a more scalable and robust system that can adapt to new tasks and data distributions. The idea is that the system would be composed of a set of `expert' models, each of which would be specialized in a particular aspect of the input data. The system would then be able to adapt to new tasks by autonomously learning a new expert model or by retraining an existing one when the need arises.
    
    The main advantage of this approach is that it would allow the system to scale to a large number of tasks and data distributions, only needing to retrain the gating function continually instead of entire models. 
    
    Another advantage is the potential for more failsafe systems. By having each model deployed independently in a distributed system, one could devise a mechanism that detects when one of the expert models fails, either due to a hardware/software error or due to a significant performance drop, and automatically replaces the gating function of the current MoE with one (previously trained) that excludes the failing model.
    
    One could also use such a mechanism as a way to save operational costs. The system might monitor the number of instances being routed to each expert model, and if one of the models is not being used, it could be automatically shut down to save resources.

    Figure \ref{fig:moes} shows a diagram illustrating the concept of a mixture of experts system.

    \begin{figure}[H]
        \centering
        \caption{Diagram of a mixture of experts model.}
        \resizebox*{0.8\linewidth}{!}{
            \input{../figures/moes_tikz.tex}
        }
        \label{fig:moes}
    \end{figure}

    % Another interesting research direction would be to explore the use of MoEs to handle multimodal data. In \cite{mustafa_multimodal_2022}, researchers at Google Brain used MoEs to train an architecture that accepts both image and text inputs and outputs a single prediction by 
  
    \subsection{Meta-Learning and L2L Systems} \label{conclusions:research_directions:l2l} \info{1 pág}

    Much like human learners, who, building from previous knowledge, continuously seek and filter information that could be useful to learn new concepts and skills, an area of research in machine learning concerns the design of programs/systems that can efficiently improve their learning process without the need for explicit human intervention. This area of research is known as \textbf{meta-learning}, and it is a very active area of research in AI \todo{add citation}.
    
    Meta-learning is a technique that aims to improve the performance of machine-learning models by `learning to learn' (L2L) a certain task. Such ideas have been successfully applied to a wide range of problems, including computer vision, natural language processing, robotics, video games, and more \cite{hospedales_meta-learning_2020}.
    
    The way meta-learning is formulated is by training a model on a variety of tasks and then using the knowledge gained from those to improve its performance on new tasks or learn it faster / more sample-efficiently than if it had been trained only for that task \cite{hospedales_meta-learning_2020}.
    
    This idea is regarded to have been first introduced by Dr. Jurgen Schmidhuber in 1987 with his thesis `Evolutionary Principles in Self-Referential Learning'. In his work, Schmidhuber proposed an algorithm that adaptively improves its learning skills by recursively applying genetic programming to itself and ensuring that only `useful' modifications (made by the program to itself) `survive' in an evolutionary fashion \cite{schmidhuber_evolutionary_1987}.
    
    Recently, Finn et al. (2017) \cite{finn_model-agnostic_2017} propose a model-agnostic framework for meta-learning that can be applied to any deep-learning architecture and learning task. The framework consists of \dots

    The idea behind researching this area is that, by adopting this L2L framework, we could design a system that can continually improve its learning process over time.

    As a low-hanging fruit, we can envision the design of a system similar to the one proposed in this work that integrates and builds upon concepts from meta-learning, unlearning, knowledge distillation, and transfer learning \footnote{See section \ref{sec:relevant_techniques}}, that enables model that can more sophisticatedly adapt to new problems. 
    
    This self-adaptive process would necessarily be based on evaluating the model's performance and a metric of the `necessity' of adaptation/learning that task better, but rather than relying on simple heuristics and a model-agnostic approach, the system would trigger a more complex adaptation process.

    \clearpage
  
    % \subsection{AutoML and Neural Architecture Search} \label{conclusions:research_directions:automl_nas}

    % AutoML and Neural Architecture Search (NAS) are other two areas of research that have been gaining a lot of traction in the last few years. The idea behind these approaches is to automate the design of machine learning models by using optimization techniques to search for the best model architecture and hyperparameters for a given task.

    

    % Elisa for example tells that one could use VAEs and models like that to discover biomarkers in voxel images of lungs that could be used to evaluate the severity of the disease, discover new patterns to diagnose the disease, or find new drugs to treat it.


    \section{Final Remarks} \label{conclusions:final_remarks} \info{0.75 pág}
    
    \dots

    Unlike many scientific advances, breakthroughs such as a true self-improving model that adapts itself continually in an online fashion will have immediate applications in every domain, from healthcare to education, to economics, to scientific discovery itself \dots 

% \printbibliography
\end{document}