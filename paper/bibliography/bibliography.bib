
@article{queralt-rosinach_applying_2022,
	title = {Applying the {FAIR} principles to data in a hospital: challenges and opportunities in a pandemic},
	volume = {13},
	issn = {2041-1480},
	shorttitle = {Applying the {FAIR} principles to data in a hospital},
	url = {https://doi.org/10.1186/s13326-022-00263-7},
	doi = {10.1186/s13326-022-00263-7},
	abstract = {The COVID-19 pandemic has challenged healthcare systems and research worldwide. Data is collected all over the world and needs to be integrated and made available to other researchers quickly. However, the various heterogeneous information systems that are used in hospitals can result in fragmentation of health data over multiple data ‘silos’ that are not interoperable for analysis. Consequently, clinical observations in hospitalised patients are not prepared to be reused efficiently and timely. There is a need to adapt the research data management in hospitals to make COVID-19 observational patient data machine actionable, i.e. more Findable, Accessible, Interoperable and Reusable (FAIR) for humans and machines. We therefore applied the FAIR principles in the hospital to make patient data more FAIR.},
	number = {1},
	urldate = {2023-01-31},
	journal = {Journal of Biomedical Semantics},
	author = {Queralt-Rosinach, Núria and Kaliyaperumal, Rajaram and Bernabé, César H. and Long, Qinqin and Joosten, Simone A. and van der Wijk, Henk Jan and Flikkenschild, Erik L.A. and Burger, Kees and Jacobsen, Annika and Mons, Barend and Roos, Marco and {BEAT-COVID Group} and {COVID-19 LUMC Group}},
	month = apr,
	year = {2022},
	keywords = {FAIR, Hospital, Ontologies, Open science, Patient data, Research data management},
	pages = {12},
	file = {Full Text PDF:/Users/simon/Zotero/storage/TVDIFFFG/Queralt-Rosinach et al. - 2022 - Applying the FAIR principles to data in a hospital.pdf:application/pdf;Snapshot:/Users/simon/Zotero/storage/TC5AHL93/s13326-022-00263-7.html:text/html},
}

@misc{noauthor_fair_nodate,
	title = {{FAIR} {Principles}},
	url = {https://www.go-fair.org/fair-principles/},
	abstract = {In 2016, the ‘FAIR Guiding Principles for scientific data management and stewardship’ were published in Scientific Data. The authors intended to provide guidelines to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets. The principles emphasise machine-actionability (i.e., the capacity of… Continue reading →},
	language = {en-US},
	urldate = {2023-01-31},
	journal = {GO FAIR},
	file = {Snapshot:/Users/simon/Zotero/storage/EV9LR3JE/fair-principles.html:text/html},
}

@article{keutzer_machine_2022,
	title = {Machine {Learning} and {Pharmacometrics} for {Prediction} of {Pharmacokinetic} {Data}: {Differences}, {Similarities} and {Challenges} {Illustrated} with {Rifampicin}},
	volume = {14},
	issn = {1999-4923},
	shorttitle = {Machine {Learning} and {Pharmacometrics} for {Prediction} of {Pharmacokinetic} {Data}},
	doi = {10.3390/pharmaceutics14081530},
	abstract = {Pharmacometrics (PM) and machine learning (ML) are both valuable for drug development to characterize pharmacokinetics (PK) and pharmacodynamics (PD). Pharmacokinetic/pharmacodynamic (PKPD) analysis using PM provides mechanistic insight into biological processes but is time- and labor-intensive. In contrast, ML models are much quicker trained, but offer less mechanistic insights. The opportunity of using ML predictions of drug PK as input for a PKPD model could strongly accelerate analysis efforts. Here exemplified by rifampicin, a widely used antibiotic, we explore the ability of different ML algorithms to predict drug PK. Based on simulated data, we trained linear regressions (LASSO), Gradient Boosting Machines, XGBoost and Random Forest to predict the plasma concentration-time series and rifampicin area under the concentration-versus-time curve from 0-24 h (AUC0-24h) after repeated dosing. XGBoost performed best for prediction of the entire PK series (R2: 0.84, root mean square error (RMSE): 6.9 mg/L, mean absolute error (MAE): 4.0 mg/L) for the scenario with the largest data size. For AUC0-24h prediction, LASSO showed the highest performance (R2: 0.97, RMSE: 29.1 h·mg/L, MAE: 18.8 h·mg/L). Increasing the number of plasma concentrations per patient (0, 2 or 6 concentrations per occasion) improved model performance. For example, for AUC0-24h prediction using LASSO, the R2 was 0.41, 0.69 and 0.97 when using predictors only (no plasma concentrations), 2 or 6 plasma concentrations per occasion as input, respectively. Run times for the ML models ranged from 1.0 s to 8 min, while the run time for the PM model was more than 3 h. Furthermore, building a PM model is more time- and labor-intensive compared with ML. ML predictions of drug PK could thus be used as input into a PKPD model, enabling time-efficient analysis.},
	language = {eng},
	number = {8},
	journal = {Pharmaceutics},
	author = {Keutzer, Lina and You, Huifang and Farnoud, Ali and Nyberg, Joakim and Wicha, Sebastian G. and Maher-Edwards, Gareth and Vlasakakis, Georgios and Moghaddam, Gita Khalili and Svensson, Elin M. and Menden, Michael P. and Simonsson, Ulrika S. H. and On Behalf Of The Unite Tb Consortium, null},
	month = jul,
	year = {2022},
	pmid = {35893785},
	pmcid = {PMC9330804},
	keywords = {machine learning, feature selection, pharmacokinetics, pharmacometrics, population pharmacokinetics, rifampicin, simulation},
	pages = {1530},
	file = {Full Text:/Users/simon/Zotero/storage/2QA59H48/Keutzer et al. - 2022 - Machine Learning and Pharmacometrics for Predictio.pdf:application/pdf},
}

@misc{noauthor_notitle_nodate,
	url = {https://wiki.xnat.org/ml},
	urldate = {2023-02-06},
	file = {https\://wiki.xnat.org/ml:/Users/simon/Zotero/storage/4NLN9GVT/ml.html:text/html},
}

@article{baeza-delgado_practical_2022,
	title = {A practical solution to estimate the sample size required for clinical prediction models generated from observational research on data},
	volume = {6},
	issn = {2509-9280},
	url = {https://doi.org/10.1186/s41747-022-00276-y},
	doi = {10.1186/s41747-022-00276-y},
	abstract = {Estimating the required sample size is crucial when developing and validating clinical prediction models. However, there is no consensus about how to determine the sample size in such a setting. Here, the goal was to compare available methods to define a practical solution to sample size estimation for clinical predictive models, as applied to Horizon 2020 PRIMAGE as a case study.},
	number = {1},
	urldate = {2023-04-22},
	journal = {European Radiology Experimental},
	author = {Baeza-Delgado, Carlos and Cerdá Alberich, Leonor and Carot-Sierra, José Miguel and Veiga-Canuto, Diana and Martínez de las Heras, Blanca and Raza, Ben and Martí-Bonmatí, Luis},
	month = jun,
	year = {2022},
	keywords = {Clinical predictive models, Paediatric oncology, PRIMAGE, Radiology, Sample size calculation},
	pages = {22},
	file = {Full Text PDF:/Users/simon/Zotero/storage/JF6XNWI3/Baeza-Delgado et al. - 2022 - A practical solution to estimate the sample size r.pdf:application/pdf},
}

@article{gulamali_autoencoders_2022,
	title = {Autoencoders for sample size estimation for fully connected neural network classifiers},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-022-00728-0},
	doi = {10.1038/s41746-022-00728-0},
	abstract = {Sample size estimation is a crucial step in experimental design but is understudied in the context of deep learning. Currently, estimating the quantity of labeled data needed to train a classifier to a desired performance, is largely based on prior experience with similar models and problems or on untested heuristics. In many supervised machine learning applications, data labeling can be expensive and time-consuming and would benefit from a more rigorous means of estimating labeling requirements. Here, we study the problem of estimating the minimum sample size of labeled training data necessary for training computer vision models as an exemplar for other deep learning problems. We consider the problem of identifying the minimal number of labeled data points to achieve a generalizable representation of the data, a minimum converging sample (MCS). We use autoencoder loss to estimate the MCS for fully connected neural network classifiers. At sample sizes smaller than the MCS estimate, fully connected networks fail to distinguish classes, and at sample sizes above the MCS estimate, generalizability strongly correlates with the loss function of the autoencoder. We provide an easily accessible, code-free, and dataset-agnostic tool to estimate sample sizes for fully connected networks. Taken together, our findings suggest that MCS and convergence estimation are promising methods to guide sample size estimates for data collection and labeling prior to training deep learning models in computer vision.},
	language = {en},
	number = {1},
	urldate = {2023-04-22},
	journal = {npj Digital Medicine},
	author = {Gulamali, Faris F. and Sawant, Ashwin S. and Kovatch, Patricia and Glicksberg, Benjamin and Charney, Alexander and Nadkarni, Girish N. and Oermann, Eric},
	month = dec,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Statistics, Epidemiology},
	pages = {1--8},
	file = {Full Text PDF:/Users/simon/Zotero/storage/A9VAC3ZH/Gulamali et al. - 2022 - Autoencoders for sample size estimation for fully .pdf:application/pdf},
}

@article{jin_quantifying_2020,
	title = {Quantifying the generalization error in deep learning in terms of data distribution and neural network smoothness},
	volume = {130},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020302392},
	doi = {10.1016/j.neunet.2020.06.024},
	abstract = {The accuracy of deep learning, i.e., deep neural networks, can be characterized by dividing the total error into three main types: approximation error, optimization error, and generalization error. Whereas there are some satisfactory answers to the problems of approximation and optimization, much less is known about the theory of generalization. Most existing theoretical works for generalization fail to explain the performance of neural networks in practice. To derive a meaningful bound, we study the generalization error of neural networks for classification problems in terms of data distribution and neural network smoothness. We introduce the cover complexity (CC) to measure the difficulty of learning a data set and the inverse of the modulus of continuity to quantify neural network smoothness. A quantitative bound for expected accuracy/error is derived by considering both the CC and neural network smoothness. Although most of the analysis is general and not specific to neural networks, we validate our theoretical assumptions and results numerically for neural networks by several data sets of images. The numerical results confirm that the expected error of trained networks scaled with the square root of the number of classes has a linear relationship with respect to the CC. We also observe a clear consistency between test loss and neural network smoothness during the training process. In addition, we demonstrate empirically that the neural network smoothness decreases when the network size increases whereas the smoothness is insensitive to training dataset size.},
	language = {en},
	urldate = {2023-04-22},
	journal = {Neural Networks},
	author = {Jin, Pengzhan and Lu, Lu and Tang, Yifa and Karniadakis, George Em},
	month = oct,
	year = {2020},
	keywords = {Neural networks, Cover complexity, Data distribution, Generalization error, Learnability, Neural network smoothness},
	pages = {85--99},
	file = {ScienceDirect Full Text PDF:/Users/simon/Zotero/storage/LF8HGD4J/Jin et al. - 2020 - Quantifying the generalization error in deep learn.pdf:application/pdf;ScienceDirect Snapshot:/Users/simon/Zotero/storage/K567FP22/S0893608020302392.html:text/html},
}

@inproceedings{alayrac_are_2019,
	title = {Are {Labels} {Required} for {Improving} {Adversarial} {Robustness}?},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bea6cfd50b4f5e3c735a972cf0eb8450-Abstract.html},
	abstract = {Recent work has uncovered the interesting (and somewhat surprising) finding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive. Our main insight is that unlabeled data can be a competitive alternative to labeled data for training adversarially robust models. Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors. On standard datasets like CIFAR- 10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7\% over using 4K supervised examples alone, and captures over 95\% of the improvement from the same number of labeled examples. Finally, we report an improvement of 4\% over the previous state-of-the- art on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset. This demonstrates that our finding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.},
	urldate = {2023-04-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Alayrac, Jean-Baptiste and Uesato, Jonathan and Huang, Po-Sen and Fawzi, Alhussein and Stanforth, Robert and Kohli, Pushmeet},
	year = {2019},
	file = {Full Text PDF:/Users/simon/Zotero/storage/96W9GULA/Alayrac et al. - 2019 - Are Labels Required for Improving Adversarial Robu.pdf:application/pdf},
}

@misc{dong_raft_2023,
	title = {{RAFT}: {Reward} {rAnked} {FineTuning} for {Generative} {Foundation} {Model} {Alignment}},
	shorttitle = {{RAFT}},
	url = {http://arxiv.org/abs/2304.06767},
	doi = {10.48550/arXiv.2304.06767},
	abstract = {Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models more effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently assembles a streaming dataset. This dataset serves as the basis for aligning the generative model and can be employed under both offline and online settings. Notably, the sample generation process within RAFT is gradient-free, rendering it compatible with black-box generators. Through extensive experiments, we demonstrate that our proposed algorithm exhibits strong performance in the context of both large language models and diffusion models.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06767 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/IIZEHKTZ/Dong et al. - 2023 - RAFT Reward rAnked FineTuning for Generative Foun.pdf:application/pdf;arXiv.org Snapshot:/Users/simon/Zotero/storage/YKGW34BD/2304.html:text/html},
}

@misc{noauthor_era4tb,
	title = {European {Accelerator} of {Tuberculosis} {Regime} {Project}},
	url = {https://era4tb.org/},
	abstract = {A public-private initiative devoted to accelerate the development of new treatment regimens for tuberculosis. Part of IMI AMR accelerator.},
	language = {en-US},
	urldate = {2023-04-24},
	journal = {ERA4TB},
	file = {Snapshot:/Users/simon/Zotero/storage/CG2I2LAV/era4tb.org.html:text/html},
}

@book{huyen_designing_2022,
	title = {Designing {Machine} {Learning} {Systems}},
    author = {Huyen, Chip},
    month=may,
    year=2022,
    publisher={O'Reilly Media, Inc.},
	url = {https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/},
	abstract = {Machine learning systems are both complex and unique. Complex because they consist of many different components and involve many different stakeholders. Unique because they're data dependent, with data varying wildly … - Selection from Designing Machine Learning Systems [Book]},
	language = {en},
	urldate = {2023-04-24},
	note = {ISBN: 9781098107963},
	file = {Snapshot:/Users/simon/Zotero/storage/EC9HG9WI/9781098107956.html:text/html},
}


@article{yakimovich_labels_2021,
	title = {Labels in a haystack: {Approaches} beyond supervised learning in biomedical applications},
	volume = {2},
	issn = {2666-3899},
	shorttitle = {Labels in a haystack},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389921002506},
	doi = {10.1016/j.patter.2021.100383},
	abstract = {Recent advances in biomedical machine learning demonstrate great potential for data-driven techniques in health care and biomedical research. However, this potential has thus far been hampered by both the scarcity of annotated data in the biomedical domain and the diversity of the domain's subfields. While unsupervised learning is capable of finding unknown patterns in the data by design, supervised learning requires human annotation to achieve the desired performance through training. With the latter performing vastly better than the former, the need for annotated datasets is high, but they are costly and laborious to obtain. This review explores a family of approaches existing between the supervised and the unsupervised problem setting. The goal of these algorithms is to make more efficient use of the available labeled data. The advantages and limitations of each approach are addressed and perspectives are provided.},
	language = {en},
	number = {12},
	urldate = {2023-04-24},
	journal = {Patterns},
	author = {Yakimovich, Artur and Beaugnon, Anaël and Huang, Yi and Ozkirimli, Elif},
	month = dec,
	year = {2021},
	keywords = {active learning, data annotation, data labeling, data value, machine learning, self-supervised learning, semi-supervised learning, zero-shot learning},
	pages = {100383},
	file = {ScienceDirect Full Text PDF:/Users/simon/Zotero/storage/GDMB362D/Yakimovich et al. - 2021 - Labels in a haystack Approaches beyond supervised.pdf:application/pdf},
}

@article{chen_study_2015,
	title = {A study of active learning methods for named entity recognition in clinical text},
	volume = {58},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046415002038},
	doi = {10.1016/j.jbi.2015.09.010},
	abstract = {Objectives
Named entity recognition (NER), a sequential labeling task, is one of the fundamental tasks for building clinical natural language processing (NLP) systems. Machine learning (ML) based approaches can achieve good performance, but they often require large amounts of annotated samples, which are expensive to build due to the requirement of domain experts in annotation. Active learning (AL), a sample selection approach integrated with supervised ML, aims to minimize the annotation cost while maximizing the performance of ML-based models. In this study, our goal was to develop and evaluate both existing and new AL methods for a clinical NER task to identify concepts of medical problems, treatments, and lab tests from the clinical notes.
Methods
Using the annotated NER corpus from the 2010 i2b2/VA NLP challenge that contained 349 clinical documents with 20,423 unique sentences, we simulated AL experiments using a number of existing and novel algorithms in three different categories including uncertainty-based, diversity-based, and baseline sampling strategies. They were compared with the passive learning that uses random sampling. Learning curves that plot performance of the NER model against the estimated annotation cost (based on number of sentences or words in the training set) were generated to evaluate different active learning and the passive learning methods and the area under the learning curve (ALC) score was computed.
Results
Based on the learning curves of F-measure vs. number of sentences, uncertainty sampling algorithms outperformed all other methods in ALC. Most diversity-based methods also performed better than random sampling in ALC. To achieve an F-measure of 0.80, the best method based on uncertainty sampling could save 66\% annotations in sentences, as compared to random sampling. For the learning curves of F-measure vs. number of words, uncertainty sampling methods again outperformed all other methods in ALC. To achieve 0.80 in F-measure, in comparison to random sampling, the best uncertainty based method saved 42\% annotations in words. But the best diversity based method reduced only 7\% annotation effort.
Conclusion
In the simulated setting, AL methods, particularly uncertainty-sampling based approaches, seemed to significantly save annotation cost for the clinical NER task. The actual benefit of active learning in clinical NER should be further evaluated in a real-time setting.},
	language = {en},
	urldate = {2023-04-24},
	journal = {Journal of Biomedical Informatics},
	author = {Chen, Yukun and Lasko, Thomas A. and Mei, Qiaozhu and Denny, Joshua C. and Xu, Hua},
	month = dec,
	year = {2015},
	keywords = {Active learning, Clinical named entity recognition, Clinical natural language processing, Machine learning},
	pages = {11--18},
	file = {ScienceDirect Full Text PDF:/Users/simon/Zotero/storage/DMIB8XKJ/Chen et al. - 2015 - A study of active learning methods for named entit.pdf:application/pdf},
}


@article{figueroa_predicting_2012,
	title = {Predicting sample size required for classification performance},
	volume = {12},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/1472-6947-12-8},
	doi = {10.1186/1472-6947-12-8},
	abstract = {Supervised learning methods need annotated data in order to generate efficient models. Annotated data, however, is a relatively scarce resource and can be expensive to obtain. For both passive and active learning methods, there is a need to estimate the size of the annotated sample required to reach a performance target.},
	number = {1},
	urldate = {2023-04-24},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Figueroa, Rosa L. and Zeng-Treitler, Qing and Kandula, Sasikiran and Ngo, Long H.},
	month = feb,
	year = {2012},
	keywords = {Active Learning, Annotate Data, Learning Curve, Mean Absolute Error, Root Mean Square Error},
	pages = {8},
	file = {Full Text PDF:/Users/simon/Zotero/storage/J5IICHCW/Figueroa et al. - 2012 - Predicting sample size required for classification.pdf:application/pdf;Snapshot:/Users/simon/Zotero/storage/YCXXRYM5/1472-6947-12-8.html:text/html},
}



@misc{christiano_deep_2023,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	doi = {10.48550/arXiv.1706.03741},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	month = feb,
	year = {2023},
	note = {arXiv:1706.03741 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}


@article{finlayson_adversarial_2019,
	title = {Adversarial attacks on medical machine learning},
	volume = {363},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7657648/},
	doi = {10.1126/science.aaw4399},
	number = {6433},
	urldate = {2023-04-25},
	journal = {Science (New York, N.Y.)},
	author = {Finlayson, Samuel G. and Bowers, John D. and Ito, Joichi and Zittrain, Jonathan L. and Beam, Andrew L. and Kohane, Isaac S.},
	month = mar,
	year = {2019},
	pmid = {30898923},
	pmcid = {PMC7657648},
	pages = {1287--1289},
}


@article{lee_clinical_2020,
	title = {Clinical applications of continual learning machine learning},
	volume = {2},
	issn = {2589-7500},
	url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30102-3/fulltext},
	doi = {10.1016/S2589-7500(20)30102-3},
	language = {English},
	number = {6},
	urldate = {2023-04-25},
	journal = {The Lancet Digital Health},
	author = {Lee, Cecilia S. and Lee, Aaron Y.},
	month = jun,
	year = {2020},
	pmid = {33328120},
	note = {Publisher: Elsevier},
	pages = {e279--e281},
}


@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2022-05-06},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
	file = {Snapshot:/Users/simon/Zotero/storage/ATKZPP5D/nature14539.html:text/html},
}


@misc{who_global_2022,
	title = {Global {Tuberculosis} {Report} 2022},
	url = {https://www.who.int/teams/global-tuberculosis-programme/tb-reports/global-tuberculosis-report-2022},
	abstract = {Global Tuberculosis Report 2022},
	author = {WHO},
	year = {2022},
	language = {en},
	urldate = {2023-08-19},
	file = {Snapshot:/Users/simon/Zotero/storage/P7RSJED7/global-tuberculosis-report-2022.html:text/html},
}

@article{zaman_tuberculosis_2010,
	title = {Tuberculosis: {A} {Global} {Health} {Problem}},
	volume = {28},
	issn = {1606-0997},
	shorttitle = {Tuberculosis},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2980871/},
	number = {2},
	urldate = {2023-08-19},
	journal = {Journal of Health, Population, and Nutrition},
	author = {Zaman, K.},
	month = apr,
	year = {2010},
	pmid = {20411672},
	pmcid = {PMC2980871},
	pages = {111--113},
	file = {PubMed Central Full Text PDF:/Users/simon/Zotero/storage/VV7QSGNJ/Zaman - 2010 - Tuberculosis A Global Health Problem.pdf:application/pdf},
}

@misc{cdctb_world_2023,
	title = {World {TB} {Day} {History}},
	url = {https://www.cdc.gov/tb/worldtbday/history.htm},
	abstract = {Each year, we recognize World TB Day on March 24.},
	language = {en-us},
	urldate = {2023-08-19},
	journal = {Centers for Disease Control and Prevention},
	author = {CDCTB},
	month = feb,
	year = {2023},
	file = {Snapshot:/Users/simon/Zotero/storage/HYP5TM2J/history.html:text/html},
}



@article{desikan_sputum_2013,
	title = {Sputum smear microscopy in tuberculosis: {Is} it still relevant?},
	volume = {137},
	issn = {0971-5916},
	shorttitle = {Sputum smear microscopy in tuberculosis},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3705651/},
	number = {3},
	urldate = {2023-08-19},
	journal = {The Indian Journal of Medical Research},
	author = {Desikan, Prabha},
	month = mar,
	year = {2013},
	pmid = {23640550},
	pmcid = {PMC3705651},
	pages = {442--444},
	file = {PubMed Central Full Text PDF:/Users/simon/Zotero/storage/GS4RGLY3/Desikan - 2013 - Sputum smear microscopy in tuberculosis Is it sti.pdf:application/pdf},
}


@misc{cdc_tb_2016,
	title = {{TB} {Diagnostic} {Tool}: {Xpert} {MTB}/{RIF} {Assay} {Fact} {Sheet} {\textbar} {TB} {\textbar} {CDC}},
	shorttitle = {{TB} {Diagnostic} {Tool}},
	authortype = {Governmental},
	author = {{CDC}: Centers for Disease Control and Prevention},
	url = {https://www.cdc.gov/tb/publications/factsheets/testing/xpert_mtb-rif.htm},
	abstract = {The Xpert MTB/RIF assay is a new test that is revolutionizing TB control by contributing to the rapid diagnosis of TB disease and drug resistance.},
	language = {en-us},
	urldate = {2023-08-19},
	month = aug,
	year = {2016},
	file = {Snapshot:/Users/simon/Zotero/storage/83JRTFD8/xpert_mtb-rif.html:text/html},
}


@article{maclean_advances_2020,
	title = {Advances in {Molecular} {Diagnosis} of {Tuberculosis}},
	volume = {58},
	url = {https://journals.asm.org/doi/10.1128/jcm.01582-19},
	doi = {10.1128/jcm.01582-19},
	abstract = {Molecular tests for tuberculosis (TB) have the potential to help reach the three million people with TB who are undiagnosed or not reported each year and to improve the quality of care TB patients receive by providing accurate, quick results, including rapid drug-susceptibility testing. The World Health Organization (WHO) has recommended the use of molecular nucleic acid amplification tests (NAATs) tests for TB detection instead of smear microscopy, as they are able to detect TB more accurately, particularly in patients with paucibacillary disease and in people living with HIV. Importantly, some of these WHO-endorsed tests can detect mycobacterial gene mutations associated with anti-TB drug resistance, allowing clinicians to tailor effective TB treatment. Currently, a wide array of molecular tests for TB detection is being developed and evaluated, and while some tests are intended for reference laboratory use, others are being aimed at the point-of-care and peripheral health care settings. Notably, there is an emergence of molecular tests designed, manufactured, and rolled out in countries with high TB burden, of which some are explicitly aimed for near-patient placement. These developments should increase access to molecular TB testing for larger patient populations. With respect to drug susceptibility testing, NAATs and next-generation sequencing can provide results substantially faster than traditional phenotypic culture. Here, we review recent advances and developments in molecular tests for detecting TB as well as anti-TB drug resistance.},
	number = {10},
	urldate = {2023-08-19},
	journal = {Journal of Clinical Microbiology},
	author = {MacLean, Emily and Kohli, Mikashmi and Weber, Stefan F. and Suresh, Anita and Schumacher, Samuel G. and Denkinger, Claudia M. and Pai, Madhukar},
	month = sep,
	year = {2020},
	note = {Publisher: American Society for Microbiology},
	pages = {10.1128/jcm.01582--19},
	file = {Full Text PDF:/Users/simon/Zotero/storage/G76DX92U/MacLean et al. - 2020 - Advances in Molecular Diagnosis of Tuberculosis.pdf:application/pdf},
}



@article{cazabon_market_2018,
	title = {Market penetration of {Xpert} {MTB}/{RIF} in high tuberculosis burden countries: {A} trend analysis from 2014 - 2016},
	volume = {2},
	issn = {2572-4754},
	shorttitle = {Market penetration of {Xpert} {MTB}/{RIF} in high tuberculosis burden countries},
	doi = {10.12688/gatesopenres.12842.2},
	abstract = {Background: Xpert® MTB/RIF, a rapid tuberculosis (TB) molecular test, was endorsed by the World Health Organization in 2010. Since then, 34.4 million cartridges have been procured under concessional pricing. Although the roll out of this diagnostic is promising, previous studies showed low market penetration. Methods: To assess 3-year trends of market penetration of Xpert MTB/RIF in the public sector, smear and Xpert MTB/RIF volumes for the year 2016 were evaluated and policies from 2014-2016 within 22 high-burden countries (HBCs) were studied. A structured questionnaire was sent to representatives of 22 HBCs. The questionnaires assessed the total smear and Xpert MTB/RIF volumes, number of modules and days of operation of GeneXpert machines in National TB Programs (NTPs). Data regarding the use of NTP GeneXpert machines for other diseases and GeneXpert procurement by other disease control programs were collected. Market penetration was estimated by the ratio of total sputum smear volume for initial diagnosis divided by the number of Xpert MTB/RIF tests procured in the public sector. Results: The survey response rate was 21/22 (95\%). Smear/Xpert ratios decreased in 17/21 countries and increased in four countries, since 2014. The median ratio decreased from 32.6 (IQR: 44.6) in 2014 to 6.0 (IQR: 15.4) in 2016. In 2016, the median GeneXpert utilization was 20\%, however seven countries (7/19; 37\%) were running tests for other diseases on their NTP-procured GeneXpert systems in 2017, such as HIV, hepatitis-C virus (HCV), Chlamydia trachomatis, and Neisseria gonorrhoeae. Five (5/15; 33\%) countries reported GeneXpert procurement by HIV or HCV programs in 2016 and/or 2017. Conclusions: Our results show a positive trend for Xpert MTB/RIF market penetration in 21 HBC public sectors. However, GeneXpert machines were under-utilized for TB, and inadequately exploited as a multi disease technology.},
	language = {eng},
	journal = {Gates Open Research},
	author = {Cazabon, Danielle and Pande, Tripti and Kik, Sandra and Van Gemert, Wayne and Sohn, Hojoon and Denkinger, Claudia and Qin, Zhi Zhen and Waning, Brenda and Pai, Madhukar},
	year = {2018},
	pmid = {30234198},
	pmcid = {PMC6139378},
	keywords = {access, diagnostics, market penetration, tuberculosis, Xpert MTB/RIF},
	pages = {35},
	file = {Full Text:/Users/simon/Zotero/storage/AT6Y5ETJ/Cazabon et al. - 2018 - Market penetration of Xpert MTBRIF in high tuberc.pdf:application/pdf},
}

@article{albert_development_2016,
	title = {Development, roll-out and impact of {Xpert} {MTB}/{RIF} for tuberculosis: what lessons have we learnt and how can we do better?},
	volume = {48},
	issn = {1399-3003},
	shorttitle = {Development, roll-out and impact of {Xpert} {MTB}/{RIF} for tuberculosis},
	doi = {10.1183/13993003.00543-2016},
	abstract = {The global roll-out of Xpert MTB/RIF (Cepheid Inc., Sunnyvale, CA, USA) has changed the diagnostic landscape of tuberculosis (TB). More than 16 million tests have been performed in 122 countries since 2011, and detection of multidrug-resistant TB has increased three- to eight-fold compared to conventional testing. The roll-out has galvanised stakeholders, from donors to civil society, and paved the way for universal drug susceptibility testing. It has attracted new product developers to TB, resulting in a robust molecular diagnostics pipeline. However, the roll-out has also highlighted gaps that have constrained scale-up and limited impact on patient outcomes. The roll-out has been hampered by high costs for under-funded programmes, unavailability of a complete solution package (notably comprehensive training, quality assurance, implementation plans, inadequate service and maintenance support) and lack of impact assessment. Insufficient focus has been afforded to effective linkage to care of diagnosed patients, and clinical impact has been blunted by weak health systems. In many countries the private sector plays a dominant role in TB control, yet this sector has limited access to subsidised pricing. In light of these lessons, we advocate for a comprehensive diagnostics implementation approach, including increased engagement of in-country stakeholders for product launch and roll-out, broader systems strengthening in preparation for new technologies, as well as quality impact data from programmatic settings.},
	language = {eng},
	number = {2},
	journal = {The European Respiratory Journal},
	author = {Albert, Heidi and Nathavitharana, Ruvandhi R. and Isaacs, Chris and Pai, Madhukar and Denkinger, Claudia M. and Boehme, Catharina C.},
	month = aug,
	year = {2016},
	pmid = {27418550},
	pmcid = {PMC4967565},
	keywords = {Antibiotics, Antitubercular, Communicable Disease Control, Drug Resistance, Bacterial, Global Health, Health Care Costs, Health Policy, Humans, Mycobacterium tuberculosis, Point-of-Care Testing, Private Sector, Quality Assurance, Health Care, Rifampin, Sputum, Tuberculosis, Multidrug-Resistant, Tuberculosis, Pulmonary},
	pages = {516--525},
	file = {Full Text:/Users/simon/Zotero/storage/QSUVFTVI/Albert et al. - 2016 - Development, roll-out and impact of Xpert MTBRIF .pdf:application/pdf},
}


@techreport{world_health_organization_regional_office_for_europe_tuberculosis_2017,
	title = {Tuberculosis: fact sheet on {Sustainable} {Development} {Goals} ({SDGs}): health targets},
	shorttitle = {Tuberculosis},
	url = {https://apps.who.int/iris/handle/10665/340885},
	language = {en},
	number = {WHO/EURO:2017-2388-42143-58059},
	urldate = {2023-08-19},
	institution = {World Health Organization. Regional Office for Europe},
	author = {{World Health Organization. Regional Office for Europe}},
	year = {2017},
	note = {number-of-pages: 8},
	keywords = {Technical documents},
	file = {Full Text PDF:/Users/simon/Zotero/storage/7N5HJ9S5/World Health Organization. Regional Office for Europe - 2017 - Tuberculosis fact sheet on Sustainable Developmen.pdf:application/pdf},
}


@misc{who_tuberculosis_2023,
	title = {Tuberculosis ({TB})},
	author = {{WHO}},
	year = {2023},
	url = {https://www.who.int/news-room/fact-sheets/detail/tuberculosis},
	abstract = {Tuberculosis is caused by bacteria that most often affect the lungs. TB is curable and preventable and is spread from person to person through the air.},
	language = {en},
	urldate = {2023-08-20},
}


@misc{imi_era4tb_2020,
	title = {{IMI} {Innovative} {Medicines} {Initiative} {\textbar} {ERA4TB} {\textbar} {European} regimen accelerator for tuberculosis},
	url = {http://www.imi.europa.eu/projects-results/project-factsheets/era4tb},
	abstract = {Tuberculosis (TB) poses a serious threat to public health worldwide; in 2018 alone, an estimated 10 million people fell ill with the disease and 1.6 million died. The goal of the ERA4TB project is to accelerate the development of a new, more efficient treatment regimen that will help the world to meet the United Nations goal of ending the TB epidemic by 2030.},
	author = {{IMI}},
	language = {en},
	urldate = {2023-08-20},
	journal = {IMI Innovative Medicines Initiative},
	month = jan,
	year = {2020},
}


@article{escalante_tuberculosis_2009,
	title = {Tuberculosis},
	volume = {150},
	issn = {0003-4819},
	url = {http://annals.org/article.aspx?doi=10.7326/0003-4819-150-11-200906020-01006},
	doi = {10.7326/0003-4819-150-11-200906020-01006},
	abstract = {Editor's Note: This issue of In the Clinic has been updated. One third of the world population has Mycobacterium tuberculosis infection (1). Despite recent progress in the United States, tuberculosis infection remains prevalent in immigrants, immunosuppressed persons, and other high-risk groups (3). Latent tuberculosis infection (LTBI) is the most prevalent form of tuberculosis in the United States (2). LTBI can progress to active tuberculosis disease, especially in individuals with a suppressed cell-mediated immunity. Active tuberculosis disease in immuno-suppressed patients can be difficult to diagnose and can progress to disseminated forms of tuberculosis disease associated with high mortality (4). New methods of diagnosing tuberculosis disease have entered practice in recent years (5), but the diagnosis of LTBI can be challenging in some high-risk populations (6, 7). The introduction of directly observed therapy with first-line antituberculous regimens (8) was an important advance in therapy, but multidrug-resistant tuberculosis (MDR-TB) and the extensively resistant form of MDR-TB remain significant threats to international and local tuberculosis control efforts (9, 10). Screening and Prevention Who should be screened for tuberculosis? Clinicians should screen all individuals at risk for tuberculosis infection, including close contacts of persons who have active pulmonary tuberculosis. Table 1 identifies asymptomatic individuals who should be screened because they are at high risk for exposure to active tuberculosis or at high risk for disease once infected. Table 1. Risk Factors for Tuberculosis Infection or Progression to Disease After Infection What tests are used to screen for tuberculosis? The tuberculin skin test (TST) with purified protein derivative (PPD) and the Mantoux method have been in use for more than 100 years to screen for tuberculosis. The TST result may not become positive for 8 to 10 weeks after exposure to active tuberculosis. The TST can give false-positive results in patient with previous bacille Calmette-Gurin (BCG) vaccination or other mycobacterial infections and false-negative results in anergic or immunosuppressed patients; however, previous BCG vaccination should not change the interpretation of the TST in most adults. The newer interferon- release assays (IGRAs), including the 2 U.S. Food and Drug Administration-approved commercial tests (T-SPOT.TB [Oxford Immunotec, Oxford, United Kingdom], and QuantiFERON-TB Gold and its In-tube version [Cellestis, Valencia, California]) can also be used in circumstances in which the TST is currently used (11). IGRAs assess the T-cell lymphocyte response to specific M. tuberculosis antigens (for example, ESAT-6 and CFP-10) and are more specific, and possibly more sensitive, than TST (12, 13). However, information about IGRA performance is limited in immunocompromised patients and patients receiving immunosuppressive therapy (6, 14). The commercially available IGRAs also have limitations; indeterminate results can occur in immunosuppressed patients, more so with QuantiFERON TB Gold than T-SPOT.TB (6). Discordant results between TST and IGRA testing also occur in about 20\% of individuals (13), which could be related, at least in part, to differences in performance characteristics of these tests (5) and to characteristics of the studied populations, such as the prevalence of persons previously vaccinated with BCG and the proportion of persons born outside the United States (15, 16). In addition to their improved specificity compared with TST, IGRAs have several practical advantages. They do not require a second visit for reading and they do not trigger amnestic responses. Longitudinal data supporting the predictive value of IGRA testing is limited, however, in contrast to the many studies of TST for predicting active tuberculosis (17). A recent study from a high-incidence area of tuberculosis in Africa found that initial test results were positive in only 56\% of TST testing and 52\% of IGRA testing in close household contacts who developed active tuberculosis during 2 years of follow up. Of these close household contacts who developed active tuberculosis, 71\% had a positive result with either TST or IGRA during their initial evaluations (18). Another prospective study (19) from a country with a low incidence of tuberculosis suggests that IGRA testing could be more accurate than TST for diagnosing LTBI and for detecting individuals who will progress to active tuberculosis, but more longitudinal data are needed, especially in immunosuppressed individuals. What can patients do to reduce their likelihood of becoming infected with tuberculosis? Tuberculosis is mainly transmitted by the airborne route from a patient with respiratory symptoms, and its ability to infect others decreases significantly after 2 weeks of effective therapy (20, 22). Therefore, prevention of tuberculosis transmission involves promptly identifying and treating patients with active tuberculosis. For hospitalized patients, prevention includes isolating patients with tuberculosis from other patients and strictly applying other hospital infection control practices (23, 24). Patients usually can be removed from airborne infection isolation when they are no longer considered infectious. Patients are no longer infectious when they are on adequate tuberculosis drug therapy, have had a significant clinical response to therapy, and have had negative results on 3 consecutive sputum smears for acid-fast bacilli (AFB). Some patients can be isolated from outsiders at home after appropriate evaluation and the initiation of outpatient treatment. Isolation of patients at home assumes that household contacts already have been exposed and that further exposure will not affect their outcomes. Two studies, one in India and one in Arkansas, showed similar rates of disease or infection in exposed household contacts whether the patient was admitted to the hospital or allowed to remain at home for initial treatment (25, 26). However, if household contacts of the patients with infectious tuberculosis are at high risk (for example, infants or immuno-compromised persons), housing the patient elsewhere until he or she meets noninfectious criteria should be strongly considered. Hospitalization may be required until housing can be obtained (27). Educating health care workers to evaluate exposed persons for active tuberculosis by obtaining sputum for AFB testing when they have respiratory symptoms has been shown to improve the case detection rates in primary care settings (28). What should clinicians tell patients with active tuberculosis to protect household members and other contacts from infection? Clinicians should teach patients to cough into disposable tissues and to cover their nose and mouth when coughing or sneezing to contain droplet nuclei before they are expelled into the air. Patients who are placed in airborne infection isolation rooms should be educated about the transmission of tuberculosis, the reasons for isolation, and the importance of staying in their rooms. Every effort should be made to help the patient follow the isolation policy (29). Hospital employees and physicians who come in contact with an infectious or suspected infectious patient should wear previously fitted particulate respirators certified by the National Institute for Occupational Safety and Health for protection against tuberculosis, which does not include surgical masks. What are the physician's public health responsibilities after making a diagnosis of active tuberculosis? All 50 U.S. states require physicians to notify public health authorities about all patients suspected of having active tuberculosis (22, 23), which can enable identification of other cases and potentially prevent further transmission of tuberculosis in the community. Genetic fingerprinting of tuberculosis isolates during an outbreak can help public health authorities detect tuberculosis infection in the community (30). Clinical Bottom Line: Screening and Prevention Clinicians should screen persons who have close contact with a person who has active pulmonary tuberculosis, and screen other persons who are at high risk for infection or for progression to disease once infected. Clinicians should screen with TST or IGRAs and should prevent infection by identifying and treating persons with active pulmonary tuberculosis. Patient airborn infection isolation is an important part of early treatment and prevention of transmission. Persons who provide care to patients with active pulmonary tuberculosis should wear particulate respirators. Clinicians should notify public health authorities about patients with suspected active tuberculosis. Diagnosis What signs and symptoms suggest active tuberculosis? Although tuberculosis can cause disease in many parts of the body, this article focuses on pulmonary tuberculosis because it is the most common form of the disease. Clinicians should consider a diagnosis of pulmonary tuberculosis and evaluate patients for tuberculosis if the patient has constitutional or pulmonary signs and symptoms, such as cough longer than 2 to 3 weeks (may not be productive until later in course of disease), hemoptysis (more likely with cavitation and rarely a presenting symptom), chest pain, fever, chills, night sweats, weight loss, easy fatigability, or anorexia. Some patients have classic signs and symptoms, but it is rare for someone to have most of the classic signs and symptoms except in advanced disease, and many patients will have few of them. Some patients with active pulmonary tuberculosis infection can be fairly asymptomatic. Table 2 shows some of the main findings from the history and the physical examination that are associated with active tuberculosis disease. Table 2. Findings from the History and Physical Examination in Patients with Active Tuberculosis One study reviewed 101 patients admitted to respiratory isolation to rul},
	language = {en},
	number = {11},
	urldate = {2023-08-20},
	journal = {Annals of Internal Medicine},
	author = {Escalante, Patricio},
	month = jun,
	year = {2009},
	pages = {ITC6--1},
	annote = {[TLDR] Clinicians should screen all individuals at risk for tuberculosis infection, including close contacts of persons who have active pulmonary tuberculosis and asymptomatic individuals who should be screened because they are at highrisk for exposure to active tuberculosis or at high risk for disease once infected.},
}


@misc{finn_model-agnostic_2017,
	title = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.03400},
	doi = {10.48550/arXiv.1703.03400},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	month = jul,
	year = {2017},
	note = {arXiv:1703.03400 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL results at https://sites.google.com/view/maml, Blog post at http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/C9MZHSQX/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf:application/pdf;arXiv.org Snapshot:/Users/simon/Zotero/storage/A8R5VB3S/1703.html:text/html},
}


@misc{hospedales_meta-learning_2020,
	title = {Meta-{Learning} in {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Meta-{Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.05439},
	doi = {10.48550/arXiv.2004.05439},
	abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
	month = nov,
	year = {2020},
	note = {arXiv:2004.05439 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv 
	
	Fulltext PDF:/Users/simon/Zotero/storage/VMGBU4WX/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf:application/pdf},
}



@misc{zhou_bert_2022,
	title = {{BERT} {Learns} to {Teach}: {Knowledge} {Distillation} with {Meta} {Learning}},
	shorttitle = {{BERT} {Learns} to {Teach}},
	url = {http://arxiv.org/abs/2106.04570},
	abstract = {We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Zhou, Wangchunshu and Xu, Canwen and McAuley, Julian},
	month = apr,
	year = {2022},
	note = {arXiv:2106.04570 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: ACL 2022 (main conference)},
	file = {arXiv.org Snapshot:/Users/simon/Zotero/storage/5G2TN85B/2106.html:text/html;Full Text PDF:/Users/simon/Zotero/storage/YAGHM438/Zhou et al. - 2022 - BERT Learns to Teach Knowledge Distillation with .pdf:application/pdf},
}


@inproceedings{kirsch_self-referential_2022,
	title = {Self-{Referential} {Meta} {Learning}},
	url = {https://openreview.net/forum?id=WAcLlCixQP7},
	abstract = {Meta Learning automates the search for learning algorithms. At the same time, it creates a dependency on human engineering on the meta-level, where meta learning algorithms need to be designed. In this paper, we investigate self-referential meta learning systems that modify themselves without the need for explicit meta optimization. We discuss the relationship of such systems to memory-based meta learning and show that self-referential neural networks require functionality to be reused in the form of parameter sharing. Finally, we propose Fitness Monotonic Execution (FME), a simple approach to avoid explicit meta optimization. A neural network self-modifies to solve bandit and classic control tasks, improves its self-modifications, and learns how to learn, purely by assigning more computational resources to better performing solutions.},
	language = {en},
	urldate = {2023-08-20},
	author = {Kirsch, Louis and Schmidhuber, Jürgen},
	month = may,
	year = {2022},
	file = {Full Text PDF:/Users/simon/Zotero/storage/6RC5UBPP/Kirsch and Schmidhuber - 2022 - Self-Referential Meta Learning.pdf:application/pdf},
}


% evolutionary principles in self-referential learning

@mastersthesis{schmidhuber_evolutionary_1987,
  added-at = {2008-06-19T17:46:40.000+0200},
  author = {Schmidhuber, Jurgen},
  biburl = {https://www.bibsonomy.org/bibtex/2a96f7c3d42103ab94b13badef5d869f0/brazovayeye},
  interhash = {b1d12416bd2edc34c30961f0ae978d8f},
  intrahash = {a96f7c3d42103ab94b13badef5d869f0},
  keywords = {EURISKO, PSALM, SALM, algorithm, algorithms, associative brigade, bucket evolution, fractals genetic genetical introsepection, learning, meta, nets, neuronal programming self-reference,},
  month = {03},
  school = {Technische Universitat Munchen, Germany},
  size = {62 pages},
  timestamp = {2008-06-19T17:51:06.000+0200},
  title = {Evolutionary Principles in Self-Referential Learning.
                 On Learning now to Learn: The Meta-Meta-Meta...-Hook},
  type = {Diploma Thesis},
  url = {http://www.idsia.ch/~juergen/diploma.html},
  year = 1987
}



@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/23UEZE4X/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf},
}


@inproceedings{visuna_novel_2023,
	title = {Novel {Deep} {Learning}-{Based} {Technique} for {Tuberculosis} {Bacilli} {Detection} in {Sputum} {Microscopy}},
	booktitle = {International {Conference} on {Interactive} {Collaborative} {Robotics}},
	publisher = {Springer},
	author = {Visuña, Lara and Garcia-Blas, Javier and Carretero, Jesus},
	year = {2023},
	pages = {269--279},
}
