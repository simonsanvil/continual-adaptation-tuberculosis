
@article{queralt-rosinach_applying_2022,
	title = {Applying the {FAIR} principles to data in a hospital: challenges and opportunities in a pandemic},
	volume = {13},
	issn = {2041-1480},
	shorttitle = {Applying the {FAIR} principles to data in a hospital},
	url = {https://doi.org/10.1186/s13326-022-00263-7},
	doi = {10.1186/s13326-022-00263-7},
	abstract = {The COVID-19 pandemic has challenged healthcare systems and research worldwide. Data is collected all over the world and needs to be integrated and made available to other researchers quickly. However, the various heterogeneous information systems that are used in hospitals can result in fragmentation of health data over multiple data ‘silos’ that are not interoperable for analysis. Consequently, clinical observations in hospitalised patients are not prepared to be reused efficiently and timely. There is a need to adapt the research data management in hospitals to make COVID-19 observational patient data machine actionable, i.e. more Findable, Accessible, Interoperable and Reusable (FAIR) for humans and machines. We therefore applied the FAIR principles in the hospital to make patient data more FAIR.},
	number = {1},
	urldate = {2023-01-31},
	journal = {Journal of Biomedical Semantics},
	author = {Queralt-Rosinach, Núria and Kaliyaperumal, Rajaram and Bernabé, César H. and Long, Qinqin and Joosten, Simone A. and van der Wijk, Henk Jan and Flikkenschild, Erik L.A. and Burger, Kees and Jacobsen, Annika and Mons, Barend and Roos, Marco and {BEAT-COVID Group} and {COVID-19 LUMC Group}},
	month = apr,
	year = {2022},
	keywords = {FAIR, Hospital, Ontologies, Open science, Patient data, Research data management},
	pages = {12},
	file = {Full Text PDF:/Users/simon/Zotero/storage/TVDIFFFG/Queralt-Rosinach et al. - 2022 - Applying the FAIR principles to data in a hospital.pdf:application/pdf;Snapshot:/Users/simon/Zotero/storage/TC5AHL93/s13326-022-00263-7.html:text/html},
}

@misc{noauthor_fair_nodate,
	title = {{FAIR} {Principles}},
	url = {https://www.go-fair.org/fair-principles/},
	abstract = {In 2016, the ‘FAIR Guiding Principles for scientific data management and stewardship’ were published in Scientific Data. The authors intended to provide guidelines to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets. The principles emphasise machine-actionability (i.e., the capacity of… Continue reading →},
	language = {en-US},
	urldate = {2023-01-31},
	journal = {GO FAIR},
	file = {Snapshot:/Users/simon/Zotero/storage/EV9LR3JE/fair-principles.html:text/html},
}

@article{keutzer_machine_2022,
	title = {Machine {Learning} and {Pharmacometrics} for {Prediction} of {Pharmacokinetic} {Data}: {Differences}, {Similarities} and {Challenges} {Illustrated} with {Rifampicin}},
	volume = {14},
	issn = {1999-4923},
	shorttitle = {Machine {Learning} and {Pharmacometrics} for {Prediction} of {Pharmacokinetic} {Data}},
	doi = {10.3390/pharmaceutics14081530},
	abstract = {Pharmacometrics (PM) and machine learning (ML) are both valuable for drug development to characterize pharmacokinetics (PK) and pharmacodynamics (PD). Pharmacokinetic/pharmacodynamic (PKPD) analysis using PM provides mechanistic insight into biological processes but is time- and labor-intensive. In contrast, ML models are much quicker trained, but offer less mechanistic insights. The opportunity of using ML predictions of drug PK as input for a PKPD model could strongly accelerate analysis efforts. Here exemplified by rifampicin, a widely used antibiotic, we explore the ability of different ML algorithms to predict drug PK. Based on simulated data, we trained linear regressions (LASSO), Gradient Boosting Machines, XGBoost and Random Forest to predict the plasma concentration-time series and rifampicin area under the concentration-versus-time curve from 0-24 h (AUC0-24h) after repeated dosing. XGBoost performed best for prediction of the entire PK series (R2: 0.84, root mean square error (RMSE): 6.9 mg/L, mean absolute error (MAE): 4.0 mg/L) for the scenario with the largest data size. For AUC0-24h prediction, LASSO showed the highest performance (R2: 0.97, RMSE: 29.1 h·mg/L, MAE: 18.8 h·mg/L). Increasing the number of plasma concentrations per patient (0, 2 or 6 concentrations per occasion) improved model performance. For example, for AUC0-24h prediction using LASSO, the R2 was 0.41, 0.69 and 0.97 when using predictors only (no plasma concentrations), 2 or 6 plasma concentrations per occasion as input, respectively. Run times for the ML models ranged from 1.0 s to 8 min, while the run time for the PM model was more than 3 h. Furthermore, building a PM model is more time- and labor-intensive compared with ML. ML predictions of drug PK could thus be used as input into a PKPD model, enabling time-efficient analysis.},
	language = {eng},
	number = {8},
	journal = {Pharmaceutics},
	author = {Keutzer, Lina and You, Huifang and Farnoud, Ali and Nyberg, Joakim and Wicha, Sebastian G. and Maher-Edwards, Gareth and Vlasakakis, Georgios and Moghaddam, Gita Khalili and Svensson, Elin M. and Menden, Michael P. and Simonsson, Ulrika S. H. and On Behalf Of The Unite Tb Consortium, null},
	month = jul,
	year = {2022},
	pmid = {35893785},
	pmcid = {PMC9330804},
	keywords = {machine learning, feature selection, pharmacokinetics, pharmacometrics, population pharmacokinetics, rifampicin, simulation},
	pages = {1530},
	file = {Full Text:/Users/simon/Zotero/storage/2QA59H48/Keutzer et al. - 2022 - Machine Learning and Pharmacometrics for Predictio.pdf:application/pdf},
}

@misc{noauthor_notitle_nodate,
	url = {https://wiki.xnat.org/ml},
	urldate = {2023-02-06},
	file = {https\://wiki.xnat.org/ml:/Users/simon/Zotero/storage/4NLN9GVT/ml.html:text/html},
}

@article{baeza-delgado_practical_2022,
	title = {A practical solution to estimate the sample size required for clinical prediction models generated from observational research on data},
	volume = {6},
	issn = {2509-9280},
	url = {https://doi.org/10.1186/s41747-022-00276-y},
	doi = {10.1186/s41747-022-00276-y},
	abstract = {Estimating the required sample size is crucial when developing and validating clinical prediction models. However, there is no consensus about how to determine the sample size in such a setting. Here, the goal was to compare available methods to define a practical solution to sample size estimation for clinical predictive models, as applied to Horizon 2020 PRIMAGE as a case study.},
	number = {1},
	urldate = {2023-04-22},
	journal = {European Radiology Experimental},
	author = {Baeza-Delgado, Carlos and Cerdá Alberich, Leonor and Carot-Sierra, José Miguel and Veiga-Canuto, Diana and Martínez de las Heras, Blanca and Raza, Ben and Martí-Bonmatí, Luis},
	month = jun,
	year = {2022},
	keywords = {Clinical predictive models, Paediatric oncology, PRIMAGE, Radiology, Sample size calculation},
	pages = {22},
	file = {Full Text PDF:/Users/simon/Zotero/storage/JF6XNWI3/Baeza-Delgado et al. - 2022 - A practical solution to estimate the sample size r.pdf:application/pdf},
}

@article{gulamali_autoencoders_2022,
	title = {Autoencoders for sample size estimation for fully connected neural network classifiers},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-022-00728-0},
	doi = {10.1038/s41746-022-00728-0},
	abstract = {Sample size estimation is a crucial step in experimental design but is understudied in the context of deep learning. Currently, estimating the quantity of labeled data needed to train a classifier to a desired performance, is largely based on prior experience with similar models and problems or on untested heuristics. In many supervised machine learning applications, data labeling can be expensive and time-consuming and would benefit from a more rigorous means of estimating labeling requirements. Here, we study the problem of estimating the minimum sample size of labeled training data necessary for training computer vision models as an exemplar for other deep learning problems. We consider the problem of identifying the minimal number of labeled data points to achieve a generalizable representation of the data, a minimum converging sample (MCS). We use autoencoder loss to estimate the MCS for fully connected neural network classifiers. At sample sizes smaller than the MCS estimate, fully connected networks fail to distinguish classes, and at sample sizes above the MCS estimate, generalizability strongly correlates with the loss function of the autoencoder. We provide an easily accessible, code-free, and dataset-agnostic tool to estimate sample sizes for fully connected networks. Taken together, our findings suggest that MCS and convergence estimation are promising methods to guide sample size estimates for data collection and labeling prior to training deep learning models in computer vision.},
	language = {en},
	number = {1},
	urldate = {2023-04-22},
	journal = {npj Digital Medicine},
	author = {Gulamali, Faris F. and Sawant, Ashwin S. and Kovatch, Patricia and Glicksberg, Benjamin and Charney, Alexander and Nadkarni, Girish N. and Oermann, Eric},
	month = dec,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Statistics, Epidemiology},
	pages = {1--8},
	file = {Full Text PDF:/Users/simon/Zotero/storage/A9VAC3ZH/Gulamali et al. - 2022 - Autoencoders for sample size estimation for fully .pdf:application/pdf},
}

@article{jin_quantifying_2020,
	title = {Quantifying the generalization error in deep learning in terms of data distribution and neural network smoothness},
	volume = {130},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020302392},
	doi = {10.1016/j.neunet.2020.06.024},
	abstract = {The accuracy of deep learning, i.e., deep neural networks, can be characterized by dividing the total error into three main types: approximation error, optimization error, and generalization error. Whereas there are some satisfactory answers to the problems of approximation and optimization, much less is known about the theory of generalization. Most existing theoretical works for generalization fail to explain the performance of neural networks in practice. To derive a meaningful bound, we study the generalization error of neural networks for classification problems in terms of data distribution and neural network smoothness. We introduce the cover complexity (CC) to measure the difficulty of learning a data set and the inverse of the modulus of continuity to quantify neural network smoothness. A quantitative bound for expected accuracy/error is derived by considering both the CC and neural network smoothness. Although most of the analysis is general and not specific to neural networks, we validate our theoretical assumptions and results numerically for neural networks by several data sets of images. The numerical results confirm that the expected error of trained networks scaled with the square root of the number of classes has a linear relationship with respect to the CC. We also observe a clear consistency between test loss and neural network smoothness during the training process. In addition, we demonstrate empirically that the neural network smoothness decreases when the network size increases whereas the smoothness is insensitive to training dataset size.},
	language = {en},
	urldate = {2023-04-22},
	journal = {Neural Networks},
	author = {Jin, Pengzhan and Lu, Lu and Tang, Yifa and Karniadakis, George Em},
	month = oct,
	year = {2020},
	keywords = {Neural networks, Cover complexity, Data distribution, Generalization error, Learnability, Neural network smoothness},
	pages = {85--99},
	file = {ScienceDirect Full Text PDF:/Users/simon/Zotero/storage/LF8HGD4J/Jin et al. - 2020 - Quantifying the generalization error in deep learn.pdf:application/pdf;ScienceDirect Snapshot:/Users/simon/Zotero/storage/K567FP22/S0893608020302392.html:text/html},
}

@inproceedings{alayrac_are_2019,
	title = {Are {Labels} {Required} for {Improving} {Adversarial} {Robustness}?},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bea6cfd50b4f5e3c735a972cf0eb8450-Abstract.html},
	abstract = {Recent work has uncovered the interesting (and somewhat surprising) finding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive. Our main insight is that unlabeled data can be a competitive alternative to labeled data for training adversarially robust models. Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors. On standard datasets like CIFAR- 10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7\% over using 4K supervised examples alone, and captures over 95\% of the improvement from the same number of labeled examples. Finally, we report an improvement of 4\% over the previous state-of-the- art on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset. This demonstrates that our finding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.},
	urldate = {2023-04-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Alayrac, Jean-Baptiste and Uesato, Jonathan and Huang, Po-Sen and Fawzi, Alhussein and Stanforth, Robert and Kohli, Pushmeet},
	year = {2019},
	file = {Full Text PDF:/Users/simon/Zotero/storage/96W9GULA/Alayrac et al. - 2019 - Are Labels Required for Improving Adversarial Robu.pdf:application/pdf},
}

@misc{dong_raft_2023,
	title = {{RAFT}: {Reward} {rAnked} {FineTuning} for {Generative} {Foundation} {Model} {Alignment}},
	shorttitle = {{RAFT}},
	url = {http://arxiv.org/abs/2304.06767},
	doi = {10.48550/arXiv.2304.06767},
	abstract = {Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models more effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently assembles a streaming dataset. This dataset serves as the basis for aligning the generative model and can be employed under both offline and online settings. Notably, the sample generation process within RAFT is gradient-free, rendering it compatible with black-box generators. Through extensive experiments, we demonstrate that our proposed algorithm exhibits strong performance in the context of both large language models and diffusion models.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06767 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/IIZEHKTZ/Dong et al. - 2023 - RAFT Reward rAnked FineTuning for Generative Foun.pdf:application/pdf;arXiv.org Snapshot:/Users/simon/Zotero/storage/YKGW34BD/2304.html:text/html},
}

@misc{noauthor_era4tb,
	title = {European {Accelerator} of {Tuberculosis} {Regime} {Project}},
	url = {https://era4tb.org/},
	abstract = {A public-private initiative devoted to accelerate the development of new treatment regimens for tuberculosis. Part of IMI AMR accelerator.},
	language = {en-US},
	urldate = {2023-04-24},
	journal = {ERA4TB},
	file = {Snapshot:/Users/simon/Zotero/storage/CG2I2LAV/era4tb.org.html:text/html},
}

@book{huyen_designing_2022,
	title = {Designing {Machine} {Learning} {Systems}},
    author = {Huyen, Chip},
    month=may,
    year=2022,
    publisher={O'Reilly Media, Inc.},
	url = {https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/},
	abstract = {Machine learning systems are both complex and unique. Complex because they consist of many different components and involve many different stakeholders. Unique because they're data dependent, with data varying wildly … - Selection from Designing Machine Learning Systems [Book]},
	language = {en},
	urldate = {2023-04-24},
	note = {ISBN: 9781098107963},
	file = {Snapshot:/Users/simon/Zotero/storage/EC9HG9WI/9781098107956.html:text/html},
}


@article{yakimovich_labels_2021,
	title = {Labels in a haystack: {Approaches} beyond supervised learning in biomedical applications},
	volume = {2},
	issn = {2666-3899},
	shorttitle = {Labels in a haystack},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389921002506},
	doi = {10.1016/j.patter.2021.100383},
	abstract = {Recent advances in biomedical machine learning demonstrate great potential for data-driven techniques in health care and biomedical research. However, this potential has thus far been hampered by both the scarcity of annotated data in the biomedical domain and the diversity of the domain's subfields. While unsupervised learning is capable of finding unknown patterns in the data by design, supervised learning requires human annotation to achieve the desired performance through training. With the latter performing vastly better than the former, the need for annotated datasets is high, but they are costly and laborious to obtain. This review explores a family of approaches existing between the supervised and the unsupervised problem setting. The goal of these algorithms is to make more efficient use of the available labeled data. The advantages and limitations of each approach are addressed and perspectives are provided.},
	language = {en},
	number = {12},
	urldate = {2023-04-24},
	journal = {Patterns},
	author = {Yakimovich, Artur and Beaugnon, Anaël and Huang, Yi and Ozkirimli, Elif},
	month = dec,
	year = {2021},
	keywords = {active learning, data annotation, data labeling, data value, machine learning, self-supervised learning, semi-supervised learning, zero-shot learning},
	pages = {100383},
	file = {ScienceDirect Full Text PDF:/Users/simon/Zotero/storage/GDMB362D/Yakimovich et al. - 2021 - Labels in a haystack Approaches beyond supervised.pdf:application/pdf},
}

@article{chen_study_2015,
	title = {A study of active learning methods for named entity recognition in clinical text},
	volume = {58},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046415002038},
	doi = {10.1016/j.jbi.2015.09.010},
	abstract = {Objectives
Named entity recognition (NER), a sequential labeling task, is one of the fundamental tasks for building clinical natural language processing (NLP) systems. Machine learning (ML) based approaches can achieve good performance, but they often require large amounts of annotated samples, which are expensive to build due to the requirement of domain experts in annotation. Active learning (AL), a sample selection approach integrated with supervised ML, aims to minimize the annotation cost while maximizing the performance of ML-based models. In this study, our goal was to develop and evaluate both existing and new AL methods for a clinical NER task to identify concepts of medical problems, treatments, and lab tests from the clinical notes.
Methods
Using the annotated NER corpus from the 2010 i2b2/VA NLP challenge that contained 349 clinical documents with 20,423 unique sentences, we simulated AL experiments using a number of existing and novel algorithms in three different categories including uncertainty-based, diversity-based, and baseline sampling strategies. They were compared with the passive learning that uses random sampling. Learning curves that plot performance of the NER model against the estimated annotation cost (based on number of sentences or words in the training set) were generated to evaluate different active learning and the passive learning methods and the area under the learning curve (ALC) score was computed.
Results
Based on the learning curves of F-measure vs. number of sentences, uncertainty sampling algorithms outperformed all other methods in ALC. Most diversity-based methods also performed better than random sampling in ALC. To achieve an F-measure of 0.80, the best method based on uncertainty sampling could save 66\% annotations in sentences, as compared to random sampling. For the learning curves of F-measure vs. number of words, uncertainty sampling methods again outperformed all other methods in ALC. To achieve 0.80 in F-measure, in comparison to random sampling, the best uncertainty based method saved 42\% annotations in words. But the best diversity based method reduced only 7\% annotation effort.
Conclusion
In the simulated setting, AL methods, particularly uncertainty-sampling based approaches, seemed to significantly save annotation cost for the clinical NER task. The actual benefit of active learning in clinical NER should be further evaluated in a real-time setting.},
	language = {en},
	urldate = {2023-04-24},
	journal = {Journal of Biomedical Informatics},
	author = {Chen, Yukun and Lasko, Thomas A. and Mei, Qiaozhu and Denny, Joshua C. and Xu, Hua},
	month = dec,
	year = {2015},
	keywords = {Active learning, Clinical named entity recognition, Clinical natural language processing, Machine learning},
	pages = {11--18},
	file = {ScienceDirect Full Text PDF:/Users/simon/Zotero/storage/DMIB8XKJ/Chen et al. - 2015 - A study of active learning methods for named entit.pdf:application/pdf},
}


@article{figueroa_predicting_2012,
	title = {Predicting sample size required for classification performance},
	volume = {12},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/1472-6947-12-8},
	doi = {10.1186/1472-6947-12-8},
	abstract = {Supervised learning methods need annotated data in order to generate efficient models. Annotated data, however, is a relatively scarce resource and can be expensive to obtain. For both passive and active learning methods, there is a need to estimate the size of the annotated sample required to reach a performance target.},
	number = {1},
	urldate = {2023-04-24},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Figueroa, Rosa L. and Zeng-Treitler, Qing and Kandula, Sasikiran and Ngo, Long H.},
	month = feb,
	year = {2012},
	keywords = {Active Learning, Annotate Data, Learning Curve, Mean Absolute Error, Root Mean Square Error},
	pages = {8},
	file = {Full Text PDF:/Users/simon/Zotero/storage/J5IICHCW/Figueroa et al. - 2012 - Predicting sample size required for classification.pdf:application/pdf;Snapshot:/Users/simon/Zotero/storage/YCXXRYM5/1472-6947-12-8.html:text/html},
}



@misc{christiano_deep_2023,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	doi = {10.48550/arXiv.1706.03741},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	month = feb,
	year = {2023},
	note = {arXiv:1706.03741 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}


@article{finlayson_adversarial_2019,
	title = {Adversarial attacks on medical machine learning},
	volume = {363},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7657648/},
	doi = {10.1126/science.aaw4399},
	number = {6433},
	urldate = {2023-04-25},
	journal = {Science (New York, N.Y.)},
	author = {Finlayson, Samuel G. and Bowers, John D. and Ito, Joichi and Zittrain, Jonathan L. and Beam, Andrew L. and Kohane, Isaac S.},
	month = mar,
	year = {2019},
	pmid = {30898923},
	pmcid = {PMC7657648},
	pages = {1287--1289},
}


@article{lee_clinical_2020,
	title = {Clinical applications of continual learning machine learning},
	volume = {2},
	issn = {2589-7500},
	url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30102-3/fulltext},
	doi = {10.1016/S2589-7500(20)30102-3},
	language = {English},
	number = {6},
	urldate = {2023-04-25},
	journal = {The Lancet Digital Health},
	author = {Lee, Cecilia S. and Lee, Aaron Y.},
	month = jun,
	year = {2020},
	pmid = {33328120},
	note = {Publisher: Elsevier},
	pages = {e279--e281},
}


@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2022-05-06},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
	file = {Snapshot:/Users/simon/Zotero/storage/ATKZPP5D/nature14539.html:text/html},
}


@misc{who_global_2022,
	title = {Global {Tuberculosis} {Report} 2022},
	url = {https://www.who.int/teams/global-tuberculosis-programme/tb-reports/global-tuberculosis-report-2022},
	abstract = {Global Tuberculosis Report 2022},
	author = {WHO},
	year = {2022},
	language = {en},
	urldate = {2023-08-19},
	file = {Snapshot:/Users/simon/Zotero/storage/P7RSJED7/global-tuberculosis-report-2022.html:text/html},
}

@article{zaman_tuberculosis_2010,
	title = {Tuberculosis: {A} {Global} {Health} {Problem}},
	volume = {28},
	issn = {1606-0997},
	shorttitle = {Tuberculosis},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2980871/},
	number = {2},
	urldate = {2023-08-19},
	journal = {Journal of Health, Population, and Nutrition},
	author = {Zaman, K.},
	month = apr,
	year = {2010},
	pmid = {20411672},
	pmcid = {PMC2980871},
	pages = {111--113},
	file = {PubMed Central Full Text PDF:/Users/simon/Zotero/storage/VV7QSGNJ/Zaman - 2010 - Tuberculosis A Global Health Problem.pdf:application/pdf},
}

@misc{cdctb_world_2023,
	title = {World {TB} {Day} {History}},
	url = {https://www.cdc.gov/tb/worldtbday/history.htm},
	abstract = {Each year, we recognize World TB Day on March 24.},
	language = {en-us},
	urldate = {2023-08-19},
	journal = {Centers for Disease Control and Prevention},
	author = {CDCTB},
	month = feb,
	year = {2023},
	file = {Snapshot:/Users/simon/Zotero/storage/HYP5TM2J/history.html:text/html},
}



@article{desikan_sputum_2013,
	title = {Sputum smear microscopy in tuberculosis: {Is} it still relevant?},
	volume = {137},
	issn = {0971-5916},
	shorttitle = {Sputum smear microscopy in tuberculosis},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3705651/},
	number = {3},
	urldate = {2023-08-19},
	journal = {The Indian Journal of Medical Research},
	author = {Desikan, Prabha},
	month = mar,
	year = {2013},
	pmid = {23640550},
	pmcid = {PMC3705651},
	pages = {442--444},
	file = {PubMed Central Full Text PDF:/Users/simon/Zotero/storage/GS4RGLY3/Desikan - 2013 - Sputum smear microscopy in tuberculosis Is it sti.pdf:application/pdf},
}


@misc{cdc_tb_2016,
	title = {{TB} {Diagnostic} {Tool}: {Xpert} {MTB}/{RIF} {Assay} {Fact} {Sheet} {\textbar} {TB} {\textbar} {CDC}},
	shorttitle = {{TB} {Diagnostic} {Tool}},
	authortype = {Governmental},
	author = {{CDC}: Centers for Disease Control and Prevention},
	url = {https://www.cdc.gov/tb/publications/factsheets/testing/xpert_mtb-rif.htm},
	abstract = {The Xpert MTB/RIF assay is a new test that is revolutionizing TB control by contributing to the rapid diagnosis of TB disease and drug resistance.},
	language = {en-us},
	urldate = {2023-08-19},
	month = aug,
	year = {2016},
	file = {Snapshot:/Users/simon/Zotero/storage/83JRTFD8/xpert_mtb-rif.html:text/html},
}


@article{maclean_advances_2020,
	title = {Advances in {Molecular} {Diagnosis} of {Tuberculosis}},
	volume = {58},
	url = {https://journals.asm.org/doi/10.1128/jcm.01582-19},
	doi = {10.1128/jcm.01582-19},
	abstract = {Molecular tests for tuberculosis (TB) have the potential to help reach the three million people with TB who are undiagnosed or not reported each year and to improve the quality of care TB patients receive by providing accurate, quick results, including rapid drug-susceptibility testing. The World Health Organization (WHO) has recommended the use of molecular nucleic acid amplification tests (NAATs) tests for TB detection instead of smear microscopy, as they are able to detect TB more accurately, particularly in patients with paucibacillary disease and in people living with HIV. Importantly, some of these WHO-endorsed tests can detect mycobacterial gene mutations associated with anti-TB drug resistance, allowing clinicians to tailor effective TB treatment. Currently, a wide array of molecular tests for TB detection is being developed and evaluated, and while some tests are intended for reference laboratory use, others are being aimed at the point-of-care and peripheral health care settings. Notably, there is an emergence of molecular tests designed, manufactured, and rolled out in countries with high TB burden, of which some are explicitly aimed for near-patient placement. These developments should increase access to molecular TB testing for larger patient populations. With respect to drug susceptibility testing, NAATs and next-generation sequencing can provide results substantially faster than traditional phenotypic culture. Here, we review recent advances and developments in molecular tests for detecting TB as well as anti-TB drug resistance.},
	number = {10},
	urldate = {2023-08-19},
	journal = {Journal of Clinical Microbiology},
	author = {MacLean, Emily and Kohli, Mikashmi and Weber, Stefan F. and Suresh, Anita and Schumacher, Samuel G. and Denkinger, Claudia M. and Pai, Madhukar},
	month = sep,
	year = {2020},
	note = {Publisher: American Society for Microbiology},
	pages = {10.1128/jcm.01582--19},
	file = {Full Text PDF:/Users/simon/Zotero/storage/G76DX92U/MacLean et al. - 2020 - Advances in Molecular Diagnosis of Tuberculosis.pdf:application/pdf},
}



@article{cazabon_market_2018,
	title = {Market penetration of {Xpert} {MTB}/{RIF} in high tuberculosis burden countries: {A} trend analysis from 2014 - 2016},
	volume = {2},
	issn = {2572-4754},
	shorttitle = {Market penetration of {Xpert} {MTB}/{RIF} in high tuberculosis burden countries},
	doi = {10.12688/gatesopenres.12842.2},
	abstract = {Background: Xpert® MTB/RIF, a rapid tuberculosis (TB) molecular test, was endorsed by the World Health Organization in 2010. Since then, 34.4 million cartridges have been procured under concessional pricing. Although the roll out of this diagnostic is promising, previous studies showed low market penetration. Methods: To assess 3-year trends of market penetration of Xpert MTB/RIF in the public sector, smear and Xpert MTB/RIF volumes for the year 2016 were evaluated and policies from 2014-2016 within 22 high-burden countries (HBCs) were studied. A structured questionnaire was sent to representatives of 22 HBCs. The questionnaires assessed the total smear and Xpert MTB/RIF volumes, number of modules and days of operation of GeneXpert machines in National TB Programs (NTPs). Data regarding the use of NTP GeneXpert machines for other diseases and GeneXpert procurement by other disease control programs were collected. Market penetration was estimated by the ratio of total sputum smear volume for initial diagnosis divided by the number of Xpert MTB/RIF tests procured in the public sector. Results: The survey response rate was 21/22 (95\%). Smear/Xpert ratios decreased in 17/21 countries and increased in four countries, since 2014. The median ratio decreased from 32.6 (IQR: 44.6) in 2014 to 6.0 (IQR: 15.4) in 2016. In 2016, the median GeneXpert utilization was 20\%, however seven countries (7/19; 37\%) were running tests for other diseases on their NTP-procured GeneXpert systems in 2017, such as HIV, hepatitis-C virus (HCV), Chlamydia trachomatis, and Neisseria gonorrhoeae. Five (5/15; 33\%) countries reported GeneXpert procurement by HIV or HCV programs in 2016 and/or 2017. Conclusions: Our results show a positive trend for Xpert MTB/RIF market penetration in 21 HBC public sectors. However, GeneXpert machines were under-utilized for TB, and inadequately exploited as a multi disease technology.},
	language = {eng},
	journal = {Gates Open Research},
	author = {Cazabon, Danielle and Pande, Tripti and Kik, Sandra and Van Gemert, Wayne and Sohn, Hojoon and Denkinger, Claudia and Qin, Zhi Zhen and Waning, Brenda and Pai, Madhukar},
	year = {2018},
	pmid = {30234198},
	pmcid = {PMC6139378},
	keywords = {access, diagnostics, market penetration, tuberculosis, Xpert MTB/RIF},
	pages = {35},
	file = {Full Text:/Users/simon/Zotero/storage/AT6Y5ETJ/Cazabon et al. - 2018 - Market penetration of Xpert MTBRIF in high tuberc.pdf:application/pdf},
}

@article{albert_development_2016,
	title = {Development, roll-out and impact of {Xpert} {MTB}/{RIF} for tuberculosis: what lessons have we learnt and how can we do better?},
	volume = {48},
	issn = {1399-3003},
	shorttitle = {Development, roll-out and impact of {Xpert} {MTB}/{RIF} for tuberculosis},
	doi = {10.1183/13993003.00543-2016},
	abstract = {The global roll-out of Xpert MTB/RIF (Cepheid Inc., Sunnyvale, CA, USA) has changed the diagnostic landscape of tuberculosis (TB). More than 16 million tests have been performed in 122 countries since 2011, and detection of multidrug-resistant TB has increased three- to eight-fold compared to conventional testing. The roll-out has galvanised stakeholders, from donors to civil society, and paved the way for universal drug susceptibility testing. It has attracted new product developers to TB, resulting in a robust molecular diagnostics pipeline. However, the roll-out has also highlighted gaps that have constrained scale-up and limited impact on patient outcomes. The roll-out has been hampered by high costs for under-funded programmes, unavailability of a complete solution package (notably comprehensive training, quality assurance, implementation plans, inadequate service and maintenance support) and lack of impact assessment. Insufficient focus has been afforded to effective linkage to care of diagnosed patients, and clinical impact has been blunted by weak health systems. In many countries the private sector plays a dominant role in TB control, yet this sector has limited access to subsidised pricing. In light of these lessons, we advocate for a comprehensive diagnostics implementation approach, including increased engagement of in-country stakeholders for product launch and roll-out, broader systems strengthening in preparation for new technologies, as well as quality impact data from programmatic settings.},
	language = {eng},
	number = {2},
	journal = {The European Respiratory Journal},
	author = {Albert, Heidi and Nathavitharana, Ruvandhi R. and Isaacs, Chris and Pai, Madhukar and Denkinger, Claudia M. and Boehme, Catharina C.},
	month = aug,
	year = {2016},
	pmid = {27418550},
	pmcid = {PMC4967565},
	keywords = {Antibiotics, Antitubercular, Communicable Disease Control, Drug Resistance, Bacterial, Global Health, Health Care Costs, Health Policy, Humans, Mycobacterium tuberculosis, Point-of-Care Testing, Private Sector, Quality Assurance, Health Care, Rifampin, Sputum, Tuberculosis, Multidrug-Resistant, Tuberculosis, Pulmonary},
	pages = {516--525},
	file = {Full Text:/Users/simon/Zotero/storage/QSUVFTVI/Albert et al. - 2016 - Development, roll-out and impact of Xpert MTBRIF .pdf:application/pdf},
}


@techreport{world_health_organization_regional_office_for_europe_tuberculosis_2017,
	title = {Tuberculosis: fact sheet on {Sustainable} {Development} {Goals} ({SDGs}): health targets},
	shorttitle = {Tuberculosis},
	url = {https://apps.who.int/iris/handle/10665/340885},
	language = {en},
	number = {WHO/EURO:2017-2388-42143-58059},
	urldate = {2023-08-19},
	institution = {World Health Organization. Regional Office for Europe},
	author = {{World Health Organization. Regional Office for Europe}},
	year = {2017},
	note = {number-of-pages: 8},
	keywords = {Technical documents},
	file = {Full Text PDF:/Users/simon/Zotero/storage/7N5HJ9S5/World Health Organization. Regional Office for Europe - 2017 - Tuberculosis fact sheet on Sustainable Developmen.pdf:application/pdf},
}


@misc{who_tuberculosis_2023,
	title = {Tuberculosis ({TB})},
	author = {{WHO}},
	year = {2023},
	url = {https://www.who.int/news-room/fact-sheets/detail/tuberculosis},
	abstract = {Tuberculosis is caused by bacteria that most often affect the lungs. TB is curable and preventable and is spread from person to person through the air.},
	language = {en},
	urldate = {2023-08-20},
}


@misc{imi_era4tb_2020,
	title = {{IMI} {Innovative} {Medicines} {Initiative} {\textbar} {ERA4TB} {\textbar} {European} regimen accelerator for tuberculosis},
	url = {http://www.imi.europa.eu/projects-results/project-factsheets/era4tb},
	abstract = {Tuberculosis (TB) poses a serious threat to public health worldwide; in 2018 alone, an estimated 10 million people fell ill with the disease and 1.6 million died. The goal of the ERA4TB project is to accelerate the development of a new, more efficient treatment regimen that will help the world to meet the United Nations goal of ending the TB epidemic by 2030.},
	author = {{IMI}},
	language = {en},
	urldate = {2023-08-20},
	journal = {IMI Innovative Medicines Initiative},
	month = jan,
	year = {2020},
}


@article{escalante_tuberculosis_2009,
	title = {Tuberculosis},
	volume = {150},
	issn = {0003-4819},
	url = {http://annals.org/article.aspx?doi=10.7326/0003-4819-150-11-200906020-01006},
	doi = {10.7326/0003-4819-150-11-200906020-01006},
	abstract = {Editor's Note: This issue of In the Clinic has been updated. One third of the world population has Mycobacterium tuberculosis infection (1). Despite recent progress in the United States, tuberculosis infection remains prevalent in immigrants, immunosuppressed persons, and other high-risk groups (3). Latent tuberculosis infection (LTBI) is the most prevalent form of tuberculosis in the United States (2). LTBI can progress to active tuberculosis disease, especially in individuals with a suppressed cell-mediated immunity. Active tuberculosis disease in immuno-suppressed patients can be difficult to diagnose and can progress to disseminated forms of tuberculosis disease associated with high mortality (4). New methods of diagnosing tuberculosis disease have entered practice in recent years (5), but the diagnosis of LTBI can be challenging in some high-risk populations (6, 7). The introduction of directly observed therapy with first-line antituberculous regimens (8) was an important advance in therapy, but multidrug-resistant tuberculosis (MDR-TB) and the extensively resistant form of MDR-TB remain significant threats to international and local tuberculosis control efforts (9, 10). Screening and Prevention Who should be screened for tuberculosis? Clinicians should screen all individuals at risk for tuberculosis infection, including close contacts of persons who have active pulmonary tuberculosis. Table 1 identifies asymptomatic individuals who should be screened because they are at high risk for exposure to active tuberculosis or at high risk for disease once infected. Table 1. Risk Factors for Tuberculosis Infection or Progression to Disease After Infection What tests are used to screen for tuberculosis? The tuberculin skin test (TST) with purified protein derivative (PPD) and the Mantoux method have been in use for more than 100 years to screen for tuberculosis. The TST result may not become positive for 8 to 10 weeks after exposure to active tuberculosis. The TST can give false-positive results in patient with previous bacille Calmette-Gurin (BCG) vaccination or other mycobacterial infections and false-negative results in anergic or immunosuppressed patients; however, previous BCG vaccination should not change the interpretation of the TST in most adults. The newer interferon- release assays (IGRAs), including the 2 U.S. Food and Drug Administration-approved commercial tests (T-SPOT.TB [Oxford Immunotec, Oxford, United Kingdom], and QuantiFERON-TB Gold and its In-tube version [Cellestis, Valencia, California]) can also be used in circumstances in which the TST is currently used (11). IGRAs assess the T-cell lymphocyte response to specific M. tuberculosis antigens (for example, ESAT-6 and CFP-10) and are more specific, and possibly more sensitive, than TST (12, 13). However, information about IGRA performance is limited in immunocompromised patients and patients receiving immunosuppressive therapy (6, 14). The commercially available IGRAs also have limitations; indeterminate results can occur in immunosuppressed patients, more so with QuantiFERON TB Gold than T-SPOT.TB (6). Discordant results between TST and IGRA testing also occur in about 20\% of individuals (13), which could be related, at least in part, to differences in performance characteristics of these tests (5) and to characteristics of the studied populations, such as the prevalence of persons previously vaccinated with BCG and the proportion of persons born outside the United States (15, 16). In addition to their improved specificity compared with TST, IGRAs have several practical advantages. They do not require a second visit for reading and they do not trigger amnestic responses. Longitudinal data supporting the predictive value of IGRA testing is limited, however, in contrast to the many studies of TST for predicting active tuberculosis (17). A recent study from a high-incidence area of tuberculosis in Africa found that initial test results were positive in only 56\% of TST testing and 52\% of IGRA testing in close household contacts who developed active tuberculosis during 2 years of follow up. Of these close household contacts who developed active tuberculosis, 71\% had a positive result with either TST or IGRA during their initial evaluations (18). Another prospective study (19) from a country with a low incidence of tuberculosis suggests that IGRA testing could be more accurate than TST for diagnosing LTBI and for detecting individuals who will progress to active tuberculosis, but more longitudinal data are needed, especially in immunosuppressed individuals. What can patients do to reduce their likelihood of becoming infected with tuberculosis? Tuberculosis is mainly transmitted by the airborne route from a patient with respiratory symptoms, and its ability to infect others decreases significantly after 2 weeks of effective therapy (20, 22). Therefore, prevention of tuberculosis transmission involves promptly identifying and treating patients with active tuberculosis. For hospitalized patients, prevention includes isolating patients with tuberculosis from other patients and strictly applying other hospital infection control practices (23, 24). Patients usually can be removed from airborne infection isolation when they are no longer considered infectious. Patients are no longer infectious when they are on adequate tuberculosis drug therapy, have had a significant clinical response to therapy, and have had negative results on 3 consecutive sputum smears for acid-fast bacilli (AFB). Some patients can be isolated from outsiders at home after appropriate evaluation and the initiation of outpatient treatment. Isolation of patients at home assumes that household contacts already have been exposed and that further exposure will not affect their outcomes. Two studies, one in India and one in Arkansas, showed similar rates of disease or infection in exposed household contacts whether the patient was admitted to the hospital or allowed to remain at home for initial treatment (25, 26). However, if household contacts of the patients with infectious tuberculosis are at high risk (for example, infants or immuno-compromised persons), housing the patient elsewhere until he or she meets noninfectious criteria should be strongly considered. Hospitalization may be required until housing can be obtained (27). Educating health care workers to evaluate exposed persons for active tuberculosis by obtaining sputum for AFB testing when they have respiratory symptoms has been shown to improve the case detection rates in primary care settings (28). What should clinicians tell patients with active tuberculosis to protect household members and other contacts from infection? Clinicians should teach patients to cough into disposable tissues and to cover their nose and mouth when coughing or sneezing to contain droplet nuclei before they are expelled into the air. Patients who are placed in airborne infection isolation rooms should be educated about the transmission of tuberculosis, the reasons for isolation, and the importance of staying in their rooms. Every effort should be made to help the patient follow the isolation policy (29). Hospital employees and physicians who come in contact with an infectious or suspected infectious patient should wear previously fitted particulate respirators certified by the National Institute for Occupational Safety and Health for protection against tuberculosis, which does not include surgical masks. What are the physician's public health responsibilities after making a diagnosis of active tuberculosis? All 50 U.S. states require physicians to notify public health authorities about all patients suspected of having active tuberculosis (22, 23), which can enable identification of other cases and potentially prevent further transmission of tuberculosis in the community. Genetic fingerprinting of tuberculosis isolates during an outbreak can help public health authorities detect tuberculosis infection in the community (30). Clinical Bottom Line: Screening and Prevention Clinicians should screen persons who have close contact with a person who has active pulmonary tuberculosis, and screen other persons who are at high risk for infection or for progression to disease once infected. Clinicians should screen with TST or IGRAs and should prevent infection by identifying and treating persons with active pulmonary tuberculosis. Patient airborn infection isolation is an important part of early treatment and prevention of transmission. Persons who provide care to patients with active pulmonary tuberculosis should wear particulate respirators. Clinicians should notify public health authorities about patients with suspected active tuberculosis. Diagnosis What signs and symptoms suggest active tuberculosis? Although tuberculosis can cause disease in many parts of the body, this article focuses on pulmonary tuberculosis because it is the most common form of the disease. Clinicians should consider a diagnosis of pulmonary tuberculosis and evaluate patients for tuberculosis if the patient has constitutional or pulmonary signs and symptoms, such as cough longer than 2 to 3 weeks (may not be productive until later in course of disease), hemoptysis (more likely with cavitation and rarely a presenting symptom), chest pain, fever, chills, night sweats, weight loss, easy fatigability, or anorexia. Some patients have classic signs and symptoms, but it is rare for someone to have most of the classic signs and symptoms except in advanced disease, and many patients will have few of them. Some patients with active pulmonary tuberculosis infection can be fairly asymptomatic. Table 2 shows some of the main findings from the history and the physical examination that are associated with active tuberculosis disease. Table 2. Findings from the History and Physical Examination in Patients with Active Tuberculosis One study reviewed 101 patients admitted to respiratory isolation to rul},
	language = {en},
	number = {11},
	urldate = {2023-08-20},
	journal = {Annals of Internal Medicine},
	author = {Escalante, Patricio},
	month = jun,
	year = {2009},
	pages = {ITC6--1},
	annote = {[TLDR] Clinicians should screen all individuals at risk for tuberculosis infection, including close contacts of persons who have active pulmonary tuberculosis and asymptomatic individuals who should be screened because they are at highrisk for exposure to active tuberculosis or at high risk for disease once infected.},
}


@misc{finn_model-agnostic_2017,
	title = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.03400},
	doi = {10.48550/arXiv.1703.03400},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	month = jul,
	year = {2017},
	note = {arXiv:1703.03400 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL results at https://sites.google.com/view/maml, Blog post at http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/C9MZHSQX/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf:application/pdf;arXiv.org Snapshot:/Users/simon/Zotero/storage/A8R5VB3S/1703.html:text/html},
}


@misc{hospedales_meta-learning_2020,
	title = {Meta-{Learning} in {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Meta-{Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.05439},
	doi = {10.48550/arXiv.2004.05439},
	abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
	month = nov,
	year = {2020},
	note = {arXiv:2004.05439 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv 
	
	Fulltext PDF:/Users/simon/Zotero/storage/VMGBU4WX/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf:application/pdf},
}



@misc{zhou_bert_2022,
	title = {{BERT} {Learns} to {Teach}: {Knowledge} {Distillation} with {Meta} {Learning}},
	shorttitle = {{BERT} {Learns} to {Teach}},
	url = {http://arxiv.org/abs/2106.04570},
	abstract = {We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Zhou, Wangchunshu and Xu, Canwen and McAuley, Julian},
	month = apr,
	year = {2022},
	note = {arXiv:2106.04570 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: ACL 2022 (main conference)},
	file = {arXiv.org Snapshot:/Users/simon/Zotero/storage/5G2TN85B/2106.html:text/html;Full Text PDF:/Users/simon/Zotero/storage/YAGHM438/Zhou et al. - 2022 - BERT Learns to Teach Knowledge Distillation with .pdf:application/pdf},
}


@inproceedings{kirsch_self-referential_2022,
	title = {Self-{Referential} {Meta} {Learning}},
	url = {https://openreview.net/forum?id=WAcLlCixQP7},
	abstract = {Meta Learning automates the search for learning algorithms. At the same time, it creates a dependency on human engineering on the meta-level, where meta learning algorithms need to be designed. In this paper, we investigate self-referential meta learning systems that modify themselves without the need for explicit meta optimization. We discuss the relationship of such systems to memory-based meta learning and show that self-referential neural networks require functionality to be reused in the form of parameter sharing. Finally, we propose Fitness Monotonic Execution (FME), a simple approach to avoid explicit meta optimization. A neural network self-modifies to solve bandit and classic control tasks, improves its self-modifications, and learns how to learn, purely by assigning more computational resources to better performing solutions.},
	language = {en},
	urldate = {2023-08-20},
	author = {Kirsch, Louis and Schmidhuber, Jürgen},
	month = may,
	year = {2022},
	file = {Full Text PDF:/Users/simon/Zotero/storage/6RC5UBPP/Kirsch and Schmidhuber - 2022 - Self-Referential Meta Learning.pdf:application/pdf},
}


% evolutionary principles in self-referential learning

@mastersthesis{schmidhuber_evolutionary_1987,
  added-at = {2008-06-19T17:46:40.000+0200},
  author = {Schmidhuber, Jurgen},
  biburl = {https://www.bibsonomy.org/bibtex/2a96f7c3d42103ab94b13badef5d869f0/brazovayeye},
  interhash = {b1d12416bd2edc34c30961f0ae978d8f},
  intrahash = {a96f7c3d42103ab94b13badef5d869f0},
  keywords = {EURISKO, PSALM, SALM, algorithm, algorithms, associative brigade, bucket evolution, fractals genetic genetical introsepection, learning, meta, nets, neuronal programming self-reference,},
  month = {03},
  school = {Technische Universitat Munchen, Germany},
  size = {62 pages},
  timestamp = {2008-06-19T17:51:06.000+0200},
  title = {Evolutionary Principles in Self-Referential Learning.
                 On Learning now to Learn: The Meta-Meta-Meta...-Hook},
  type = {Diploma Thesis},
  url = {http://www.idsia.ch/~juergen/diploma.html},
  year = 1987
}



@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/23UEZE4X/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf},
}


@inproceedings{visuna_novel_2023,
	title = {Novel {Deep} {Learning}-{Based} {Technique} for {Tuberculosis} {Bacilli} {Detection} in {Sputum} {Microscopy}},
	booktitle = {International {Conference} on {Interactive} {Collaborative} {Robotics}},
	publisher = {Springer},
	author = {Visuña, Lara and Garcia-Blas, Javier and Carretero, Jesus},
	year = {2023},
	pages = {269--279},
}


@article{boehme_rapid_2010,
	title = {Rapid {Molecular} {Detection} of {Tuberculosis} and {Rifampin} {Resistance}},
	volume = {363},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMoa0907847},
	doi = {10.1056/NEJMoa0907847},
	abstract = {Only a small fraction of the estimated 500,000 patients who have multidrug-resistant tuberculosis and 1.37 million patients who have coinfection with tuberculosis and the human immunodeficiency virus (HIV) worldwide each year have access to sufficiently sensitive case detection or drug-susceptibility testing.1 Diagnostic delay, aggravated by the disproportionate frequency of smear-negative disease in HIV-associated tuberculosis, is common.2–5 The failure to quickly recognize and treat affected patients leads to increased mortality, secondary resistance (including extensively drug-resistant tuberculosis), and ongoing transmission.6,7 The complexity of mycobacterial culture and current nucleic acid–amplification technologies for the detection of tuberculosis and multidrug-resistant tuberculosis8 and the . . .},
	number = {11},
	urldate = {2023-08-22},
	journal = {New England Journal of Medicine},
	author = {Boehme, Catharina C. and Nabeta, Pamela and Hillemann, Doris and Nicol, Mark P. and Shenai, Shubhada and Krapp, Fiorella and Allen, Jenny and Tahirli, Rasim and Blakemore, Robert and Rustomjee, Roxana and Milovic, Ana and Jones, Martin and O'Brien, Sean M. and Persing, David H. and Ruesch-Gerdes, Sabine and Gotuzzo, Eduardo and Rodrigues, Camilla and Alland, David and Perkins, Mark D.},
	month = sep,
	year = {2010},
	pmid = {20825313},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/NEJMoa0907847},
	pages = {1005--1015},
	file = {Full Text PDF:/Users/simon/Zotero/storage/MHSDZZTC/Boehme et al. - 2010 - Rapid Molecular Detection of Tuberculosis and Rifa.pdf:application/pdf},
}


@article{osman_tuberculosis_2011,
	title = {Tuberculosis bacilli detection in {Ziehl}-{Neelsen}-stained tissue using affine moment invariants and {Extreme} {Learning} {Machine}},
	url = {http://ieeexplore.ieee.org/document/5759878/},
	doi = {10.1109/CSPA.2011.5759878},
	abstract = {This paper describes an approach to automate the detection and classification of tuberculosis (TB) bacilli in tissue section using image processing technique and feedforward neural network trained by Extreme Learning Machine. It aims to assist pathologists in TB diagnosis and give an alternative to the conventional manual screening process, which is time-consuming and labour-intensive. Images are captured from Ziehl-Neelsen (ZN) stained tissue slides using light microscope as it is commonly used approach for diagnosis of TB. Then colour image segmentation is used to locate the regions correspond to the bacilli. After that, affine moment invariants are extracted to represent the segmented regions. These features are invariant under rotation, scale and translation, thus useful to represent the bacilli. Finally, a single layer feedforward neural network (SLFNN) trained by Extreme Learning Machine (ELM) is used to detect and classify the features into three classes: ‘TB’, ‘overlapped TB’ and ‘non-TB’. The results indicate that the ELM gives acceptable classification performance with shorter training period compared to the standard backpropagation training algorithms.},
	urldate = {2023-08-22},
	journal = {2011 IEEE 7th International Colloquium on Signal Processing and its Applications},
	author = {Osman, M. K. and Mashor, M. Y. and Jaafar, H.},
	month = mar,
	year = {2011},
	note = {Conference Name: its Applications (CSPA)
ISBN: 9781612844145
Place: Penang, Malaysia
Publisher: IEEE},
	pages = {232--236},
	annote = {[TLDR] An approach to automate the detection and classification of tuberculosis (TB) bacilli in tissue section using image processing technique and feedforward neural network trained by Extreme Learning Machine to assist pathologists in TB diagnosis and give an alternative to the conventional manual screening process.},
}

@inproceedings{el-melegy_identification_2019,
	title = {Identification of {Tuberculosis} {Bacilli} in {ZN}-{Stained} {Sputum} {Smear} {Images}: {A} {Deep} {Learning} {Approach}},
	shorttitle = {Identification of {Tuberculosis} {Bacilli} in {ZN}-{Stained} {Sputum} {Smear} {Images}},
	doi = {10.1109/CVPRW.2019.00147},
	abstract = {Tuberculosis (TB) is a serious infectious disease that remains a global health problem with an enormous burden of disease. TB spreads widely in low and middle income countries, which depend primarily on ZN-stained sputum smear test using conventional light microscopy in disease diagnosis. In this paper we propose a new deep-learning approach for bacilli localization and classification in conventional ZN-stained microscopic images. The new approach is based on the state of the art Faster Region-based Convolutional Neural Network (RCNN) framework, followed by a CNN to reduce false positive rate. This is the first time to apply this framework to this problem. Our experimental results show significant improvement by the proposed approach compared to existing methods, which will help in accurate disease diagnosis.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {El-Melegy, Moumen and Mohamed, Doaa and ElMelegy, Tarek and Abdelrahman, Mostafa},
	month = jun,
	year = {2019},
	note = {ISSN: 2160-7516},
	keywords = {Feature extraction, Image color analysis, Image segmentation, Machine learning, Microscopy, Proposals, Training},
	pages = {1131--1137},
}


@article{shah_ziehlneelsen_2017,
	title = {Ziehl–{Neelsen} sputum smear microscopy image database: a resource to facilitate automated bacilli detection for tuberculosis diagnosis},
	volume = {4},
	issn = {2329-4302},
	shorttitle = {Ziehl–{Neelsen} sputum smear microscopy image database},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5492794/},
	doi = {10.1117/1.JMI.4.2.027503},
	abstract = {Ziehl–Neelsen stained microscopy is a crucial bacteriological test for tuberculosis detection, but its sensitivity is poor. According to the World Health Organization (WHO) recommendation, 300 viewfields should be analyzed to augment sensitivity, but only a few viewfields are examined due to patient load. Therefore, tuberculosis diagnosis through automated capture of the focused image (autofocusing), stitching of viewfields to form mosaics (autostitching), and automatic bacilli segmentation (grading) can significantly improve the sensitivity. However, the lack of unified datasets impedes the development of robust algorithms in these three domains. Therefore, the Ziehl–Neelsen sputum smear microscopy image database (ZNSM iDB) has been developed, and is freely available. This database contains seven categories of diverse datasets acquired from three different bright-field microscopes. Datasets related to autofocusing, autostitching, and manually segmenting bacilli can be used for developing algorithms, whereas the other four datasets are provided to streamline the sensitivity and specificity. All three categories of datasets were validated using different automated algorithms. As images available in this database have distinctive presentations with high noise and artifacts, this referral resource can also be used for the validation of robust detection algorithms. The ZNSM-iDB also assists for the development of methods in automated microscopy.},
	number = {2},
	urldate = {2023-08-22},
	journal = {Journal of Medical Imaging},
	author = {Shah, Mohammad Imran and Mishra, Smriti and Yadav, Vinod Kumar and Chauhan, Arun and Sarkar, Malay and Sharma, Sudarshan K. and Rout, Chittaranjan},
	month = apr,
	year = {2017},
	pmid = {28680911},
	pmcid = {PMC5492794},
	pages = {027503},
	file = {PubMed Central Full Text PDF:/Users/simon/Zotero/storage/YY65LQHA/Shah et al. - 2017 - Ziehl–Neelsen sputum smear microscopy image databa.pdf:application/pdf},
}


@article{ubaidi_radiological_nodate,
	title = {The {Radiological} {Diagnosis} of {Pulmonary} {Tuberculosis} ({TB}) in {Primary} {Care}},
	issn = {2469-5793},
	year = {2018},
	month = {03},
	url = {https://www.clinmedjournals.org/articles/jfmdp/journal-of-family-medicine-and-disease-prevention-jfmdp-4-073.php?jid=jfmdp},
	doi = {10.23937/2469-5793/1510073},
	abstract = {The Bahrain screening program depends primarly on the use of chest x-ray and PPD, while not using both symptom inquiry and Xpert MTB/RIF (XP). The essential keys are to teach and train all physicians in the detection of early symptoms with x-ray findings of active, inactive and diagnose latent pulmonary tuberculosis.},
	language = {en-US},
	urldate = {2023-08-22},
	author = {Ubaidi, Basem Abbas Al},
	note = {Publisher: clinmed journals},
	file = {Full Text:/Users/simon/Zotero/storage/NF2SVI96/Ubaidi - The Radiological Diagnosis of Pulmonary Tuberculos.pdf:application/pdf},
}


@article{zhu_semi-supervised_2008,
	title = {Semi-{Supervised} {Learning} {Literature} {Survey}},
	volume = {2},
	abstract = {We review the literature on semi-supervised learning, which is an area in machine learning and more generally, artificial intelligence. There has been a whole
spectrum of interesting ideas on how to learn from both labeled and unlabeled data, i.e. semi-supervised learning. This document is a chapter excerpt from the author’s
doctoral thesis (Zhu, 2005). However the author plans to update the online version frequently to incorporate the latest development in the field. Please obtain the latest
version at http://www.cs.wisc.edu/{\textasciitilde}jerryzhu/pub/ssl\_survey.pdf},
	journal = {Comput Sci, University of Wisconsin-Madison},
	author = {Zhu, Xiaojin},
	month = jul,
	year = {2008},
}


@article{joshi_federated_2022,
	title = {Federated {Learning} for {Healthcare} {Domain} - {Pipeline}, {Applications} and {Challenges}},
	volume = {3},
	issn = {2691-1957, 2637-8051},
	url = {https://dl.acm.org/doi/10.1145/3533708},
	doi = {10.1145/3533708},
	abstract = {Federated learning is the process of developing machine learning models over datasets distributed across data centers such as hospitals, clinical research labs, and mobile devices while preventing data leakage. This survey examines previous research and studies on federated learning in the healthcare sector across a range of use cases and applications. Our survey shows what challenges, methods, and applications a practitioner should be aware of in the topic of federated learning. This paper aims to lay out existing research and list the possibilities of federated learning for healthcare industries.},
	language = {en},
	number = {4},
	urldate = {2023-08-22},
	journal = {ACM Transactions on Computing for Healthcare},
	author = {Joshi, Madhura and Pal, Ankit and Sankarasubbu, Malaikannan},
	month = oct,
	year = {2022},
	pages = {1--36},
	file = {Full Text PDF:/Users/simon/Zotero/storage/R5PG33I6/Joshi et al. - 2022 - Federated Learning for Healthcare Domain - Pipelin.pdf:application/pdf},
}


@misc{chen_towards_2022,
	title = {Towards {Understanding} {Mixture} of {Experts} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2208.02813},
	doi = {10.48550/arXiv.2208.02813},
	abstract = {The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by a router, has achieved great success in deep learning. However, the understanding of such architecture remains elusive. In this paper, we formally study how the MoE layer improves the performance of neural network learning and why the mixture model will not collapse into a single model. Our empirical results suggest that the cluster structure of the underlying problem and the non-linearity of the expert are pivotal to the success of MoE. To further understand this, we consider a challenging classification problem with intrinsic cluster structures, which is hard to learn using a single expert. Yet with the MoE layer, by choosing the experts as two-layer nonlinear convolutional neural networks (CNNs), we show that the problem can be learned successfully. Furthermore, our theory shows that the router can learn the cluster-center features, which helps divide the input complex problem into simpler linear classification sub-problems that individual experts can conquer. To our knowledge, this is the first result towards formally understanding the mechanism of the MoE layer for deep learning.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Chen, Zixiang and Deng, Yihe and Wu, Yue and Gu, Quanquan and Li, Yuanzhi},
	month = aug,
	year = {2022},
	note = {arXiv:2208.02813 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 53 pages, 8 figures, 11 tables},
}

@misc{mustafa_multimodal_2022,
	title = {Multimodal {Contrastive} {Learning} with {LIMoE}: the {Language}-{Image} {Mixture} of {Experts}},
	shorttitle = {Multimodal {Contrastive} {Learning} with {LIMoE}},
	url = {http://arxiv.org/abs/2206.02770},
	doi = {10.48550/arXiv.2206.02770},
	abstract = {Large sparsely-activated models have obtained excellent performance in multiple domains. However, such models are typically trained on a single modality at a time. We present the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning. LIMoE accepts both images and text simultaneously, while being trained using a contrastive loss. MoEs are a natural fit for a multimodal backbone, since expert layers can learn an appropriate partitioning of modalities. However, new challenges arise; in particular, training stability and balanced expert utilization, for which we propose an entropy-based regularization scheme. Across multiple scales, we demonstrate remarkable performance improvement over dense models of equivalent computational cost. LIMoE-L/16 trained comparably to CLIP-L/14 achieves 78.6\% zero-shot ImageNet accuracy (vs. 76.2\%), and when further scaled to H/14 (with additional data) it achieves 84.1\%, comparable to state-of-the-art methods which use larger custom per-modality backbones and pre-training schemes. We analyse the quantitative and qualitative behavior of LIMoE, and demonstrate phenomena such as differing treatment of the modalities and the organic emergence of modality-specific experts.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Mustafa, Basil and Riquelme, Carlos and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil},
	month = jun,
	year = {2022},
	note = {arXiv:2206.02770 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}


@misc{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Networks} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	shorttitle = {Deep {Compression}},
	url = {http://arxiv.org/abs/1510.00149},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	month = feb,
	year = {2016},
	note = {arXiv:1510.00149 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published as a conference paper at ICLR 2016 (oral)},
	file = {arXiv.org Snapshot:/Users/simon/Zotero/storage/WHDELQV4/1510.html:text/html;Full Text PDF:/Users/simon/Zotero/storage/6DAQSAIX/Han et al. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf:application/pdf},
}



@inproceedings{carreira-perpinan_compression_2018,
	title = {"{Learning}-{Compression}" {Algorithms} for {Neural} {Net} {Pruning}},
	doi = {10.1109/CVPR.2018.00890},
	abstract = {Pruning a neural net consists of removing weights without degrading its performance. This is an old problem of renewed interest because of the need to compress ever larger nets so they can run in mobile devices. Pruning has been traditionally done by ranking or penalizing weights according to some criterion (such as magnitude), removing low-ranked weights and retraining the remaining ones. We formulate pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition. We give a generic algorithm to solve this which alternates "learning" steps that optimize a regularized, data-dependent loss and "compression" steps that mark weights for pruning in a data-independent way. Magnitude thresholding arises naturally in the compression step, but unlike existing magnitude pruning approaches, our algorithm explores subsets of weights rather than committing irrevocably to a specific subset from the beginning. It is also able to learn automatically the best number of weights to prune in each layer of the net without incurring an exponentially costly model selection. Using a single pruning-level user parameter, we achieve state-of-the-art pruning in LeNet and ResNets of various sizes.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Carreira-Perpinan, Miguel A. and Idelbayev, Yerlan},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Mobile handsets, Neural networks, Neurons, Optimization, Performance evaluation, Quantization (signal), Training},
	pages = {8532--8541},
	file = {IEEE Xplore Full Text PDF:/Users/simon/Zotero/storage/IJJBUDL5/Carreira-Perpinan and Idelbayev - 2018 - Learning-Compression Algorithms for Neural Net P.pdf:application/pdf},
}



@misc{carreira-perpinan_model_2017,
	title = {Model compression as constrained optimization, with application to neural nets. {Part} {II}: quantization},
	shorttitle = {Model compression as constrained optimization, with application to neural nets. {Part} {II}},
	url = {http://arxiv.org/abs/1707.04319},
	doi = {10.48550/arXiv.1707.04319},
	abstract = {We consider the problem of deep neural net compression by quantization: given a large, reference net, we want to quantize its real-valued weights using a codebook with \$K\$ entries so that the training loss of the quantized net is minimal. The codebook can be optimally learned jointly with the net, or fixed, as for binarization or ternarization approaches. Previous work has quantized the weights of the reference net, or incorporated rounding operations in the backpropagation algorithm, but this has no guarantee of converging to a loss-optimal, quantized net. We describe a new approach based on the recently proposed framework of model compression as constrained optimization {\textbackslash}citep\{Carreir17a\}. This results in a simple iterative "learning-compression" algorithm, which alternates a step that learns a net of continuous weights with a step that quantizes (or binarizes/ternarizes) the weights, and is guaranteed to converge to local optimum of the loss for quantized nets. We develop algorithms for an adaptive codebook or a (partially) fixed codebook. The latter includes binarization, ternarization, powers-of-two and other important particular cases. We show experimentally that we can achieve much higher compression rates than previous quantization work (even using just 1 bit per weight) with negligible loss degradation.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Carreira-Perpiñán, Miguel Á and Idelbayev, Yerlan},
	month = jul,
	year = {2017},
	note = {arXiv:1707.04319 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, Statistics - Machine Learning},
	annote = {Comment: 33 pages, 15 figures, 3 tables},
}



@incollection{casimiro_self-adaptive_2022,
	title = {Self-adaptive {Machine} {Learning} {Systems}: {Research} {Challenges} and {Opportunities}},
	isbn = {978-3-031-15115-6},
	shorttitle = {Self-adaptive {Machine} {Learning} {Systems}},
	abstract = {Today’s world is witnessing a shift from human-written software to machine-learned software, with the rise of systems that rely on machine learning. These systems typically operate in non-static environments, which are prone to unexpected changes, as is the case of self-driving cars and enterprise systems. In this context, machine-learned software can misbehave. Thus, it is paramount that these systems are capable of detecting problems with their machined-learned components and adapting themselves to maintain desired qualities. For instance, a fraud detection system that cannot adapt its machine-learned model to efficiently cope with emerging fraud patterns or changes in the volume of transactions is subject to losses of millions of dollars. In this paper, we take a first step towards the development of a framework for self-adaptation of systems that rely on machine-learned components. We describe: (i) a set of causes of machine-learned component misbehavior and a set of adaptation tactics inspired by the literature on machine learning, motivating them with the aid of two running examples from the enterprise systems and cyber-physical systems domains; (ii) the required changes to the MAPE-K loop, a popular control loop for self-adaptive systems; and (iii) the challenges associated with developing this framework. We conclude with a set of research questions to guide future work.KeywordsSelf-adaptive systemsMachine learningModel degradationLearning-enabled systemsLearning-enabled components},
	author = {Casimiro, Maria and Romano, Paolo and Garlan, David and Moreno, Gabriel and Kang, Eunsuk and Klein, Mark},
	month = aug,
	year = {2022},
	doi = {10.1007/978-3-031-15116-3_7},
	pages = {133--155},
	file = {Full Text PDF:/Users/simon/Zotero/storage/MF23QCFV/Casimiro et al. - 2022 - Self-adaptive Machine Learning Systems Research C.pdf:application/pdf},
}


@misc{shazeer_outrageously_2017,
	title = {Outrageously {Large} {Neural} {Networks}: {The} {Sparsely}-{Gated} {Mixture}-of-{Experts} {Layer}},
	shorttitle = {Outrageously {Large} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1701.06538v1},
	abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
	language = {en},
	urldate = {2023-08-23},
	journal = {arXiv.org},
	author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	month = jan,
	year = {2017},
	file = {Full Text PDF:/Users/simon/Zotero/storage/Z8NGZWY6/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-G.pdf:application/pdf},
}


@misc{reisser_federated_2021,
	title = {Federated {Mixture} of {Experts}},
	url = {http://arxiv.org/abs/2107.06724},
	abstract = {Federated learning (FL) has emerged as the predominant approach for collaborative training of neural network models across multiple users, without the need to gather the data at a central location. One of the important challenges in this setting is data heterogeneity, i.e. different users have different data characteristics. For this reason, training and using a single global model might be suboptimal when considering the performance of each of the individual user’s data. In this work, we tackle this problem via Federated Mixture of Experts, FedMix, a framework that allows us to train an ensemble of specialized models. FedMix adaptively selects and trains a user-speciﬁc selection of the ensemble members. We show that users with similar data characteristics select the same members and therefore share statistical strength while mitigating the effect of non-i.i.d data. Empirically, we show through an extensive experimental evaluation that FedMix improves performance compared to using a single global model across a variety of different sources of non-i.i.d.-ness.},
	language = {en},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Reisser, Matthias and Louizos, Christos and Gavves, Efstratios and Welling, Max},
	month = jul,
	year = {2021},
	note = {arXiv:2107.06724 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {Reisser et al. - 2021 - Federated Mixture of Experts.pdf:/Users/simon/Zotero/storage/MDRAICJ8/Reisser et al. - 2021 - Federated Mixture of Experts.pdf:application/pdf},
}


@misc{minoura_scmm_2021,
	title = {{ScMM}: {Mixture}-of-{Experts} multimodal deep generative model for single-cell multiomics data analysis},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{ScMM}},
	url = {https://www.biorxiv.org/content/10.1101/2021.02.18.431907v1},
	doi = {10.1101/2021.02.18.431907},
	abstract = {The recent development in single-cell multiomics analysis has enabled simultaneous detection of multiple traits at the single-cell level, thus providing deeper insights into the cellular phenotypes and functions in diverse tissues. However, currently, it is challenging to infer the joint representations and learn relationships among multiple modalities from complex multimodal single-cell data. Herein, we present scMM, a novel deep generative model-based framework for the extraction of interpretable joint representations and cross-modal generation. scMM addresses the complexity of data by leveraging a mixture-of-experts multimodal variational autoencoder. The pseudocell generation strategy of scMM compensates for the limited interpretability of deep learning models and discovered multimodal regulatory programs associated with latent dimensions. Analysis of recently produced datasets validated that scMM facilitates high-resolution clustering with rich interpretability. Furthermore, we show that cross-modal generation by scMM leads to more precise prediction and data integration compared with the state-of-the-art and conventional approaches.},
	language = {en},
	urldate = {2023-08-22},
	publisher = {bioRxiv},
	author = {Minoura, Kodai and Abe, Ko and Nam, Hyunha and Nishikawa, Hiroyoshi and Shimamura, Teppei},
	month = feb,
	year = {2021},
	note = {Pages: 2021.02.18.431907
Section: New Results},
	file = {Full Text PDF:/Users/simon/Zotero/storage/F2NYW5NM/Minoura et al. - 2021 - ScMM Mixture-of-Experts multimodal deep generativ.pdf:application/pdf},
}


@misc{mustafa_multimodal_2022,
	title = {Multimodal {Contrastive} {Learning} with {LIMoE}: the {Language}-{Image} {Mixture} of {Experts}},
	shorttitle = {Multimodal {Contrastive} {Learning} with {LIMoE}},
	url = {http://arxiv.org/abs/2206.02770},
	doi = {10.48550/arXiv.2206.02770},
	abstract = {Large sparsely-activated models have obtained excellent performance in multiple domains. However, such models are typically trained on a single modality at a time. We present the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning. LIMoE accepts both images and text simultaneously, while being trained using a contrastive loss. MoEs are a natural fit for a multimodal backbone, since expert layers can learn an appropriate partitioning of modalities. However, new challenges arise; in particular, training stability and balanced expert utilization, for which we propose an entropy-based regularization scheme. Across multiple scales, we demonstrate remarkable performance improvement over dense models of equivalent computational cost. LIMoE-L/16 trained comparably to CLIP-L/14 achieves 78.6\% zero-shot ImageNet accuracy (vs. 76.2\%), and when further scaled to H/14 (with additional data) it achieves 84.1\%, comparable to state-of-the-art methods which use larger custom per-modality backbones and pre-training schemes. We analyse the quantitative and qualitative behavior of LIMoE, and demonstrate phenomena such as differing treatment of the modalities and the organic emergence of modality-specific experts.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Mustafa, Basil and Riquelme, Carlos and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil},
	month = jun,
	year = {2022},
	note = {arXiv:2206.02770 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/8BKRNHTR/Mustafa et al. - 2022 - Multimodal Contrastive Learning with LIMoE the La.pdf:application/pdf;arXiv.org Snapshot:/Users/simon/Zotero/storage/FETDHUJ4/2206.html:text/html},
}


@misc{hwang_tutel_2023,
	title = {Tutel: {Adaptive} {Mixture}-of-{Experts} at {Scale}},
	shorttitle = {Tutel},
	url = {http://arxiv.org/abs/2206.03382},
	abstract = {Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep learning models to trillion-plus parameters with fixed computational cost. The algorithmic performance of MoE relies on its token routing mechanism that forwards each input token to the right sub-models or experts. While token routing dynamically determines the amount of expert workload at runtime, existing systems suffer inefficient computation due to their static execution, namely static parallelism and pipelining, which does not adapt to the dynamic workload. We present Flex, a highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining. Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence or tensor migration overhead. This enables adaptive parallelism/pipelining optimization at zero cost during runtime. Based on this key design, Flex also implements various MoE acceleration techniques. Aggregating all techniques, Flex finally delivers huge speedup at any scale -- 4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs, respectively, over the previous state-of-the-art. Our evaluation shows that Flex efficiently and effectively runs a real-world MoE-based model named SwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision architecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x and 2.11x speedup in training and inference over Fairseq, respectively. On effectiveness, the SwinV2-MoE model achieves superior accuracy in both pre-training and down-stream computer vision tasks such as COCO object detection than the counterpart dense model, indicating the readiness of Flex for end-to-end real-world model training and inference.},
	urldate = {2023-07-14},
	publisher = {arXiv},
	author = {Hwang, Changho and Cui, Wei and Xiong, Yifan and Yang, Ziyue and Liu, Ze and Hu, Han and Wang, Zilong and Salas, Rafael and Jose, Jithin and Ram, Prabhat and Chau, Joe and Cheng, Peng and Yang, Fan and Yang, Mao and Xiong, Yongqiang},
	month = jun,
	year = {2023},
	note = {arXiv:2206.03382 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/Users/simon/Zotero/storage/8LH2DBTW/2206.html:text/html;Full Text PDF:/Users/simon/Zotero/storage/7HHCF4FV/Hwang et al. - 2023 - Tutel Adaptive Mixture-of-Experts at Scale.pdf:application/pdf},
}



@misc{noauthor_ethics_2019,
	title = {Ethics guidelines for trustworthy {AI} {\textbar} {Shaping} {Europe}’s digital future},
	url = {https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai},
	abstract = {On 8 April 2019, the High-Level Expert Group on AI presented Ethics Guidelines for Trustworthy Artificial Intelligence. This followed the publication of the guidelines' first draft in December 2018 on which more than 500 comments were received through an open consultation.},
	author = {{European Commission}},
	language = {en},
	urldate = {2023-08-23},
	month = apr,
	year = {2019},
}


@misc{park_trak_2023,
	title = {{TRAK}: {Attributing} {Model} {Behavior} at {Scale}},
	shorttitle = {{TRAK}},
	url = {http://arxiv.org/abs/2303.14186},
	doi = {10.48550/arXiv.2303.14186},
	abstract = {The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets. In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scales: image classifiers trained on ImageNet, vision-language models (CLIP), and language models (BERT and mT5). We provide code for using TRAK (and reproducing our work) at https://github.com/MadryLab/trak .},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Park, Sung Min and Georgiev, Kristian and Ilyas, Andrew and Leclerc, Guillaume and Madry, Aleksander},
	month = apr,
	year = {2023},
	note = {arXiv:2303.14186 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/3ICJEL7L/Park et al. - 2023 - TRAK Attributing Model Behavior at Scale.pdf:application/pdf;arXiv.org Snapshot:/Users/simon/Zotero/storage/BAYS6HC2/2303.html:text/html},
}


@misc{holzmuller_framework_2023,
	title = {A {Framework} and {Benchmark} for {Deep} {Batch} {Active} {Learning} for {Regression}},
	url = {http://arxiv.org/abs/2203.09410},
	doi = {10.48550/arXiv.2203.09410},
	abstract = {The acquisition of labels for supervised learning can be expensive. To improve the sample efficiency of neural network regression, we study active learning methods that adaptively select batches of unlabeled data for labeling. We present a framework for constructing such methods out of (network-dependent) base kernels, kernel transformations, and selection methods. Our framework encompasses many existing Bayesian methods based on Gaussian process approximations of neural networks as well as non-Bayesian methods. Additionally, we propose to replace the commonly used last-layer features with sketched finite-width neural tangent kernels and to combine them with a novel clustering method. To evaluate different methods, we introduce an open-source benchmark consisting of 15 large tabular regression data sets. Our proposed method outperforms the state-of-the-art on our benchmark, scales to large data sets, and works out-of-the-box without adjusting the network architecture or training code. We provide open-source code that includes efficient implementations of all kernels, kernel transformations, and selection methods, and can be used for reproducing our results.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Holzmüller, David and Zaverkin, Viktor and Kästner, Johannes and Steinwart, Ingo},
	month = aug,
	year = {2023},
	note = {arXiv:2203.09410 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Published at the Journal of Machine Learning Research (JMLR). Changes in v4: Improvements in writing and other minor changes. Accompanying code can be found at https://github.com/dholzmueller/bmdal\_reg},
}


@misc{liu_influence_2021,
	title = {Influence {Selection} for {Active} {Learning}},
	url = {http://arxiv.org/abs/2108.09331},
	doi = {10.48550/arXiv.2108.09331},
	abstract = {The existing active learning methods select the samples by evaluating the sample's uncertainty or its effect on the diversity of labeled datasets based on different task-specific or model-specific criteria. In this paper, we propose the Influence Selection for Active Learning(ISAL) which selects the unlabeled samples that can provide the most positive Influence on model performance. To obtain the Influence of the unlabeled sample in the active learning scenario, we design the Untrained Unlabeled sample Influence Calculation(UUIC) to estimate the unlabeled sample's expected gradient with which we calculate its Influence. To prove the effectiveness of UUIC, we provide both theoretical and experimental analyses. Since the UUIC just depends on the model gradients, which can be obtained easily from any neural network, our active learning algorithm is task-agnostic and model-agnostic. ISAL achieves state-of-the-art performance in different active learning settings for different tasks with different datasets. Compared with previous methods, our method decreases the annotation cost at least by 12\%, 13\% and 16\% on CIFAR10, VOC2012 and COCO, respectively.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Liu, Zhuoming and Ding, Hao and Zhong, Huaping and Li, Weijia and Dai, Jifeng and He, Conghui},
	month = aug,
	year = {2021},
	note = {arXiv:2108.09331 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: ICCV2021 accepted paper},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/D3URHFBR/Liu et al. - 2021 - Influence Selection for Active Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/simon/Zotero/storage/2GEXE8VD/2108.html:text/html},
}


@misc{eu_aiact_2023,
	title = {{EU} {AI} {Act}: first regulation on artificial intelligence {\textbar} {News} {\textbar} {European} {Parliament}},
	shorttitle = {{EU} {AI} {Act}},
	url = {https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence},
	abstract = {The use of artificial intelligence in the EU will be regulated by the AI Act, the world’s first comprehensive AI law. Find out how it will protect you.},
	language = {en},
	author = {{European Parliament}},
	urldate = {2023-08-23},
	month = aug,
	year = {2023},
	file = {Snapshot:/Users/simon/Zotero/storage/MSN4WILI/eu-ai-act-first-regulation-on-artificial-intelligence.html:text/html},
}


@article{esteva_deep_2021,
	title = {Deep learning-enabled medical computer vision},
	volume = {4},
	issn = {2398-6352},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7794558/},
	doi = {10.1038/s41746-020-00376-2},
	abstract = {A decade of unprecedented progress in artificial intelligence (AI) has demonstrated the potential for many fields—including medicine—to benefit from the insights that AI techniques can extract from data. Here we survey recent progress in the development of modern computer vision techniques—powered by deep learning—for medical applications, focusing on medical imaging, medical video, and clinical deployment. We start by briefly summarizing a decade of progress in convolutional neural networks, including the vision tasks they enable, in the context of healthcare. Next, we discuss several example medical imaging applications that stand to benefit—including cardiology, pathology, dermatology, ophthalmology–and propose new avenues for continued work. We then expand into general medical video, highlighting ways in which clinical workflows can integrate computer vision to enhance care. Finally, we discuss the challenges and hurdles required for real-world clinical deployment of these technologies.},
	urldate = {2023-08-25},
	journal = {NPJ Digital Medicine},
	author = {Esteva, Andre and Chou, Katherine and Yeung, Serena and Naik, Nikhil and Madani, Ali and Mottaghi, Ali and Liu, Yun and Topol, Eric and Dean, Jeff and Socher, Richard},
	month = jan,
	year = {2021},
	pmid = {33420381},
	pmcid = {PMC7794558},
	pages = {5},
}

@article{topol_high-performance_2019,
	title = {High-performance medicine: the convergence of human and artificial intelligence},
	volume = {25},
	copyright = {2019 Springer Nature America, Inc.},
	issn = {1546-170X},
	shorttitle = {High-performance medicine},
	url = {https://www.nature.com/articles/s41591-018-0300-7},
	doi = {10.1038/s41591-018-0300-7},
	abstract = {The use of artificial intelligence, and the deep-learning subtype in particular, has been enabled by the use of labeled big data, along with markedly enhanced computing power and cloud storage, across all sectors. In medicine, this is beginning to have an impact at three levels: for clinicians, predominantly via rapid, accurate image interpretation; for health systems, by improving workflow and the potential for reducing medical errors; and for patients, by enabling them to process their own data to promote health. The current limitations, including bias, privacy and security, and lack of transparency, along with the future directions of these applications will be discussed in this article. Over time, marked improvements in accuracy, productivity, and workflow will likely be actualized, but whether that will be used to improve the patient–doctor relationship or facilitate its erosion remains to be seen.},
	language = {en},
	number = {1},
	urldate = {2023-08-25},
	journal = {Nature Medicine},
	author = {Topol, Eric J.},
	month = jan,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Health care, Machine learning},
	pages = {44--56},
}



@inproceedings{wang_chestx-ray8_2017,
	title = {{ChestX}-ray8: {Hospital}-scale {Chest} {X}-ray {Database} and {Benchmarks} on {Weakly}-{Supervised} {Classification} and {Localization} of {Common} {Thorax} {Diseases}},
	shorttitle = {{ChestX}-ray8},
	url = {http://arxiv.org/abs/1705.02315},
	doi = {10.1109/CVPR.2017.369},
	abstract = {The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely "ChestX-ray8", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based "reading chest X-rays" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems. Data download link: https://nihcc.app.box.com/v/ChestXray-NIHCC},
	urldate = {2023-08-25},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
	month = jul,
	year = {2017},
	note = {arXiv:1705.02315 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	pages = {3462--3471},
	annote = {Comment: CVPR 2017 spotlight;V1: CVPR submission+supplementary; V2: Statistics and benchmark results on published ChestX-ray14 dataset are updated in Appendix B V3: Minor correction V4: new data download link upated: https://nihcc.app.box.com/v/ChestXray-NIHCC V5: Update benchmark results on the published data split in the appendix},
	file = {arXiv Fulltext PDF:/Users/simon/Zotero/storage/4JCKTFMR/Wang et al. - 2017 - ChestX-ray8 Hospital-scale Chest X-ray Database a.pdf:application/pdf;arXiv.org Snapshot:/Users/simon/Zotero/storage/9MUSV4X2/1705.html:text/html},
}
